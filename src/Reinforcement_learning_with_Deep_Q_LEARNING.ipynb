{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCEMENT LEARNING WITH DEEP Q-LEARNING\n",
    "\n",
    "## DEEP Q-LEARNING OVERVIEW\n",
    "\n",
    "The logic of the Deep _Q-learning algorithm_ is very similar to _Q-learning_, but in this case, the policy or _Q-table_ is not learned directly through the _Q-function_. Instead, it is learned by a neural network.\n",
    "\n",
    "A neural network is defined such that, given a state, it returns the best Q(s, a) value across all possible actions, selecting only those with the highest Q(s, a) values. The network consists of:\n",
    "\n",
    "- **Input Layer**: Represents the necessary neurons to encode the agent's current state (In 2D grid example, x and y coordinates, so 2 neurons).\n",
    "- **Output Layer**: Encodes the actions the agent can take in the environment (In 2D grid example, 4 actions: up, down, left, and right).\n",
    "- **Intermediate Layers**: Determined based on the complexity of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Add the parent directory (where \"modules\" is located) to the Python path\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from modules import environment_creation_functions as ecf\n",
    "from modules import environment as env\n",
    "from modules import deep_q_learner as dqlearn\n",
    "from modules import deep_q_learning_env_agent_execution as dqae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD ENVIRONMENT\n",
    "\n",
    "Initial state of the agent is represented in green color, the final or destination state in red and barrier in black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating maze\n",
      "Grid of: 4 x 4\n",
      "Barriers count: 4\n",
      "Source: (0, 0)\n",
      "Destination: (3, 3)\n",
      "Path to destination found.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create environment\n",
    "dim, grid_matrix_positions, barriers, src, dest, reward_matrix  = ecf.create_environment(False, 4, 5) # adjust max values\n",
    "environment = env.Environment(reward_matrix, list(src), list(dest)) # convert states from tuple into list\n",
    "\n",
    "# visualize created environment\n",
    "%matplotlib inline\n",
    "ecf.visualize_environment(dim, barriers, src, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEEP Q-LEARNING ALGORITHM\n",
    "\n",
    "### LEARNING PROCESS\n",
    "\n",
    "The learning process in the Deep Q-learning algorithm does not occur per action, as it does in the `Q-learning` or `SARSA` algorithms. Instead, it occurs over entire episodes, where the algorithm takes actions and stores them in memory. This memory is called `Experience Replay`, and for each action, it records:\n",
    "\n",
    "- the **current state**,\n",
    "- the **action taken**,\n",
    "- the **reward** obtained,\n",
    "- the **next state**, and\n",
    "- whether or not **it is a terminal state**.\n",
    "\n",
    "Once the memory is filled, a set of actions is selected (i.e.: randomly), and a model is trained to adjust the weights, allowing the network to learn from each action.\n",
    "\n",
    "- Algorithm pseudocode:\n",
    "\n",
    "   ```\n",
    "   # Initialize Memory (Experience Replay) : Current State | Taken Action | Reward | Next State | Is final state?\n",
    "   # Set memory size i.e. mem_size = 200\n",
    "   # Neuronal Network initialization\n",
    "   For each episode do:\n",
    "        s = initial state\n",
    "        actions_per_episode = 0\n",
    "        memory = empty\n",
    "        while (s != not final) and (actions_per_episode < mem_size) do:\n",
    "            a = Choose action. Cases:\n",
    "                - random(action) if random() >= exploration_rate \n",
    "                - Predict (Neuronal network): Q_predict = max_a Q(St,a)  if random() < exploration_rate  \n",
    "            Execute action a           \n",
    "            R(S_t, a) = reward\n",
    "            S_t = S_t+1 # New state\n",
    "            save_in_memory = S_t, a, R(S_t, a), S_t+1, is_final_state?\n",
    "        batch = select actions from memory (randomly)\n",
    "        for each element in batch do:\n",
    "            Predict (Neuronal network) = Q_predict(S_t, a)            \n",
    "            Q_obj(S_t, a) =  R(S_t, a)  + (gamma . max_a'Q(S_t+1_, a'))     \n",
    "            Train Neuronal Netwok (X= S_t, Y = Q_obj(S_t, a), loss = MSE) \n",
    "                Loss => 1/2 Sum(TD(S_t,a)^2) # temporal difference error\n",
    "        end\n",
    "    end\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHORT-TERM FOCUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through episodes. Evolution prints will appear every 10 episodes.\n",
      "EPISODE 10 - Actions: 66 - Reward: -1186.0\n",
      "New expl.ratio: 0.47. Best reward now: -726.0. Best n_step now: 16\n",
      "EPISODE 20 - Actions: 7 - Reward: -16.0\n",
      "New expl.ratio: 0.63. Best reward now: 64.0. Best n_step now: 6\n",
      "EPISODE 30 - Actions: 9 - Reward: 42.0\n",
      "New expl.ratio: 0.71. Best reward now: 94.0. Best n_step now: 6\n",
      "EPISODE 40 - Actions: 8 - Reward: -6.0\n",
      "New expl.ratio: 0.76. Best reward now: 94.0. Best n_step now: 6\n",
      "EPISODE 50 - Actions: 6 - Reward: 64.0\n",
      "New expl.ratio: 0.79. Best reward now: 94.0. Best n_step now: 6\n",
      "EPISODE 60 - Actions: 6 - Reward: 94.0\n",
      "New expl.ratio: 0.81. Best reward now: 94.0. Best n_step now: 6\n",
      "EPISODE 70 - Actions: 6 - Reward: 64.0\n",
      "New expl.ratio: 0.83. Best reward now: 94.0. Best n_step now: 6\n",
      "EPISODE 80 - Actions: 6 - Reward: 94.0\n",
      "New expl.ratio: 0.84. Best reward now: 94.0. Best n_step now: 6\n",
      "EPISODE 90 - Actions: 6 - Reward: 64.0\n",
      "New expl.ratio: 0.85. Best reward now: 94.0. Best n_step now: 6\n",
      "EPISODE 100 - Actions: 8 - Reward: 62.0\n",
      "New expl.ratio: 0.86. Best reward now: 94.0. Best n_step now: 6\n",
      "\n",
      "Q_TABLE:\n",
      "State        Up      Down      Left     Right\n",
      "x0,y0 -1.576318 -0.659719 -1.807855 -0.127845\n",
      "x0,y1 -2.287448  0.170668 -2.525862 -0.424536\n",
      "x0,y2 -3.081134  0.709664 -3.554312 -0.692648\n",
      "x0,y3 -3.874819  1.248660 -4.582761 -0.960759\n",
      "x1,y0 -2.353561 -1.198997 -3.135396  0.048332\n",
      "x1,y1 -3.024887 -0.351250 -3.657614 -0.092021\n",
      "x1,y2 -3.694328  0.506060 -4.309954 -0.461070\n",
      "x1,y3 -4.454732  1.043795 -5.304530 -0.747619\n",
      "x2,y0 -3.208391 -1.729110 -4.519855  0.248546\n",
      "x2,y1 -3.897428 -0.993933 -5.015770  0.197718\n",
      "x2,y2 -4.538230 -0.042714 -5.575725 -0.054942\n",
      "x2,y3 -5.203137  0.832001 -6.199639 -0.444696\n",
      "x3,y0 -4.065259 -2.259050 -5.904164  0.447863\n",
      "x3,y1 -4.737301 -1.587449 -6.431050  0.417399\n",
      "x3,y2 -5.436099 -0.722203 -6.881896  0.326801\n",
      "x3,y3 -6.057587  0.256024 -7.496931 -0.030570\n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "          y0        y1        y2        y3\n",
      "x0 -0.127845  0.170668  0.709664  1.248660\n",
      "x1  0.048332 -0.092021  0.506060  1.043795\n",
      "x2  0.248546  0.197718 -0.042714  0.832001\n",
      "x3  0.447863  0.417399  0.326801  0.256024\n",
      "\n",
      "BEST ACTIONS:\n",
      "       y0     y1     y2    y3\n",
      "x0  Right   Down   Down  Down\n",
      "x1  Right  Right   Down  Down\n",
      "x2  Right  Right   Down  Down\n",
      "x3  Right  Right  Right  Down\n",
      "\n",
      "STEPS: \n",
      "   [[0, 1], [1, 1], [1, 2], [2, 2], [3, 2], [3, 3]]\n",
      "\n",
      "PATH:\n",
      "   y0 y1 y2 y3\n",
      "x0  0  1  -  -\n",
      "x1  -  2  3  -\n",
      "x2  -  -  4  -\n",
      "x3  -  -  5  6\n"
     ]
    }
   ],
   "source": [
    "# Apply agent deep q-learning and print results\n",
    "dqae.agent_deep_q_learning(learner=dqlearn.DeepQLearner,                           \n",
    "                           environment = deepcopy(environment),  # Agent same environment.\n",
    "                           num_episodes=100, # Note: adjust if grid is greater.\n",
    "                           discount_factor=0.1, # Near 0, learn to move to the next most rewarding state\n",
    "                           # Aim to obtain 95% explotation, during the learning process is adjusted.\n",
    "                           # Starts from exploration and gradually goes to explotation.\n",
    "                           explotation_ratio=0.95, # Greedy control\n",
    "                           max_steps= 500, \n",
    "                           verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LONG-TERM FOCUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through episodes. Evolution prints will appear every 10 episodes.\n",
      "EPISODE 10 - Actions: 28 - Reward: -606.0\n",
      "New expl.ratio: 0.47. Best reward now: -120.0. Best n_step now: 10\n",
      "EPISODE 20 - Actions: 20 - Reward: -228.0\n",
      "New expl.ratio: 0.63. Best reward now: 92.0. Best n_step now: 8\n",
      "EPISODE 30 - Actions: 8 - Reward: 92.0\n",
      "New expl.ratio: 0.71. Best reward now: 92.0. Best n_step now: 8\n",
      "EPISODE 40 - Actions: 9 - Reward: 42.0\n",
      "New expl.ratio: 0.76. Best reward now: 94.0. Best n_step now: 6\n",
      "EPISODE 50 - Actions: 6 - Reward: 94.0\n",
      "New expl.ratio: 0.79. Best reward now: 94.0. Best n_step now: 6\n",
      "EPISODE 60 - Actions: 8 - Reward: 62.0\n",
      "New expl.ratio: 0.81. Best reward now: 94.0. Best n_step now: 6\n",
      "EPISODE 70 - Actions: 8 - Reward: 92.0\n",
      "New expl.ratio: 0.83. Best reward now: 94.0. Best n_step now: 6\n"
     ]
    }
   ],
   "source": [
    "# Apply agent deep q-learning and print results\n",
    "dqae.agent_deep_q_learning(learner=dqlearn.DeepQLearner,\n",
    "                           environment = deepcopy(environment), # Agent same environment.\n",
    "                           num_episodes=100, # Note: adjust if grid is greater.\n",
    "                           discount_factor=0.9, # Near 1, move with a view to maximizing the final reward\n",
    "                           # Aim to obtain 95% explotation, during the learning process is adjusted.\n",
    "                           # Starts from exploration and gradually goes to explotation.\n",
    "                           explotation_ratio=0.95,   # Greedy control,\n",
    "                           max_steps= 500, \n",
    "                           verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
