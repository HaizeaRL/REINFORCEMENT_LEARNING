{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCEMENT LEARNING WITH DEEP Q-LEARNING\n",
    "\n",
    "## DEEP Q-LEARNING OVERVIEW\n",
    "\n",
    "The logic of the Deep _Q-learning algorithm_ is very similar to _Q-learning_, but in this case, the policy or _Q-table_ is not learned directly through the _Q-function_. Instead, it is learned by a neural network.\n",
    "\n",
    "A neural network is defined such that, given a state, it returns the best Q(s, a) value across all possible actions, selecting only those with the highest Q(s, a) values. The network consists of:\n",
    "\n",
    "- **Input Layer**: Represents the necessary neurons to encode the agent's current state (In 2D grid example, x and y coordinates, so 2 neurons).\n",
    "- **Output Layer**: Encodes the actions the agent can take in the environment (In 2D grid example, 4 actions: up, down, left, and right).\n",
    "- **Intermediate Layers**: Determined based on the complexity of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Add the parent directory (where \"modules\" is located) to the Python path\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from modules import environment_creation_functions as ecf\n",
    "from modules import environment as env\n",
    "from modules import deep_q_learner as dqlearn\n",
    "from modules import deep_q_learning_env_agent_execution as dqae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD ENVIRONMENT\n",
    "\n",
    "Initial state of the agent is represented in green color, the final or destination state in red and barrier in black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating maze\n",
      "Grid of: 4 x 4\n",
      "Barriers count: 4\n",
      "Source: (2, 0)\n",
      "Destination: (1, 3)\n",
      "Failed to find the destination cell\n",
      "Creating maze\n",
      "Grid of: 5 x 5\n",
      "Barriers count: 6\n",
      "Source: (4, 0)\n",
      "Destination: (2, 4)\n",
      "Path to destination found.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAFICAYAAAA24bcOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAGaklEQVR4nO3bMY7j2BVA0S+jMIDMzpV7F1pA7cjL8I64C+9CedMCJqEDJ4KB9qV6XKKmeU6k4KPwHqi6qE+gTuu6rgOAH/rL3gMAvDuhBAhCCRCEEiAIJUAQSoAglABBKAGCUAKEj60HT6fTV84B8HJb/zFxcyg5nnmex/V63XuMl1iWZVwulzHGGLfbbUzTtPNEr/G4Nz8mlPzQ+Xw+TDAeTdN0yL35Me8oAYJQAgShBAhCCRCEEiAIJUAQSoAglABBKAGCUAIEoQQIQgkQhBIgCCVAEEqAIJQAQSgBglACBKEECEIJEIQSIAglQBBKgCCUAEEoAYJQAgShBAhCCRCEEiAIJUAQSoAglABBKAGCUAIEoQQIQgkQhBIgCCVAEEqAIJQAQSgBglACBKEECEIJEIQSIAglQBBKgCCUAEEoAYJQAgShBAhCCRCEEiAIJUAQSoAglABBKAHCx94D8L7u9/tYlmXvMV7icc+j7DzGsXb9I07ruq6bDp5OXz0LwEttzJ+rN0B56uo9z/M4n89fNctbud/v4/Pzc4xx3L1vt9uYpmnniV5jWZZxuVzGGMfd+6jf8y2eunp///79UF+gb9++jTGGvQ/A3sfc29Ub4P9EKAGCUAIEoQQIQgkQhBIgCCVAEEqAIJQAQSgBglACBKEECEIJEIQSIAglQBBKgCCUAEEoAYJQAgShBAhCCRCEEiAIJUAQSoAglABBKAGCUAIEoQQIQgkQhBIgCCVAEEqAIJQAQSgBglACBKEECEIJEIQSIAglQBBKgCCUAEEoAYJQAgShBAhCCRCEEiAIJUAQSoAglABBKAGCUAIEoQQIQgkQhBIgCCVAEEqA8PHM4WVZvmqOt/O4q71/ffY+7t5bnNZ1XTcdPJ1+aiCAd7Uxf67eAOWpq/dRzfM8rtfr3mO8xLIs43K5jDHGuN1uY5qmnSd6DXuPcRtjHGPrMZYxxuWJ80K5wfl8PswvzqNpmux9INM4Tiif5eoNEIQSIAglQBBKgCCUAEEoAYJQAgShBAhCCRCEEiAIJUAQSoAglABBKAGCUAIEoQQIQgkQhBIgCCVAEEqAIJQAQSgBglACBKEECEIJEIQSIAglQBBKgCCUAEEoAYJQAgShBAhCCRCEEiAIJUAQSoAglABBKAGCUAIEoQQIQgkQhBIgCCVAEEqAIJQAQSgBglACBKEECEIJEIQSIAglQBBKgCCUAEEoAYJQAgShBAhCCRA+9h7gz+B+v49lWfYe4yUe9zzKzmPYe4wxjrP187ue1nVdNx08nX5iHID3tTF/rt4AxdV7g3mex/V63XuMl1iWZVwul73H2NVRn/dtnsd0Pu880Wss9/u4fH5uPi+UG5zP5zFN095j8CJHfd7T9XqcvZ98D+3qDRCEEiAIJUAQSoAglABBKAGCUAIEoQQIQgkQhBIgCCVAEEqAIJQAQSgBglACBKEECEIJEIQSIAglQBBKgCCUAEEoAYJQAgShBAhCCRCEEiAIJUAQSoAglABBKAGCUAIEoQQIQgkQhBIgCCVAEEqAIJQAQSgBglACBKEECEIJEIQSIAglQBBKgCCUAEEoAYJQAgShBAhCCRCEEiAIJUAQSoAglABBKAGCUAIEoQQIH3sP8Gdwv9/Hsix7j/ESR9nzfznq8z7KzmM8v+tpXdd108HT6acGAnhXG/Pn6g1Qnrt6/32M8dvXDPJ2fh9j/OM/H+d/zuP6t+uu47zKsizjcrmMMcaY53mcz+edJ3qN+/0+Pj8/xxhj3G63MU3TzhO9xuPzPureWzwXyt/GcUL54PzX82G+QI+u1+th9n58ZzVN02H2fnTUvbdw9QYIQgkQhBIgCCVAEEqAIJQAQSgBglACBKEECEIJEIQSIAglQBBKgCCUAEEoAYJQAgShBAhCCRCEEiAIJUAQSoAglABBKAGCUAIEoQQIQgkQhBIgCCVAEEqAIJQAQSgBglACBKEECEIJEIQSIAglQBBKgCCUAEEoAYJQAgShBAhCCRCEEiAIJUAQSoAglABBKAGCUAIEoQQIQgkQhBIgCCVAEEqAIJQAQSgBglACBKEECB9Pnf79i6Z4Rw+73v91H8uy7DfLCz3ueZSdx7D3f3/+1T2762ld13XTwdPppwYCeFcb87f9L8qtPxDgV+MdJUAQSoAglABBKAGCUAIEoQQIQgkQhBIgCCVA+Dc3WDtaws4qYAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create environment\n",
    "dim, grid_matrix_positions, barriers, src, dest, reward_matrix  = ecf.create_environment(False, 4, 6) \n",
    "environment = env.Environment(reward_matrix, list(src), list(dest)) # convert states from tuple into list\n",
    "\n",
    "# visualize created environment\n",
    "%matplotlib inline\n",
    "ecf.visualize_environment(dim, barriers, src, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEEP Q-LEARNING ALGORITHM\n",
    "\n",
    "### LEARNING PROCESS\n",
    "\n",
    "The learning process in the Deep Q-learning algorithm does not occur per action, as it does in the `Q-learning` or `SARSA` algorithms. Instead, it occurs over entire episodes, where the algorithm takes actions and stores them in memory. This memory is called `Experience Replay`, and for each action, it records:\n",
    "\n",
    "- the **current state**,\n",
    "- the **action taken**,\n",
    "- the **reward** obtained,\n",
    "- the **next state**, and\n",
    "- whether or not **it is a terminal state**.\n",
    "\n",
    "Once the memory is filled, a set of actions is selected (i.e.: randomly), and a model is trained to adjust the weights, allowing the network to learn from each action.\n",
    "\n",
    "- Algorithm pseudocode:\n",
    "\n",
    "   ```\n",
    "   # Initialize Memory (Experience Replay) : Current State | Taken Action | Reward | Next State | Is final state?\n",
    "   # Set memory size i.e. mem_size = 200\n",
    "   # Neuronal Network initialization\n",
    "   For each episode do:\n",
    "        s = initial state\n",
    "        actions_per_episode = 0\n",
    "        memory = empty\n",
    "        while (s != not final) and (actions_per_episode < mem_size) do:\n",
    "            a = Choose action. Cases:\n",
    "                - random(action) if random() >= exploration_rate \n",
    "                - Predict (Neuronal network): Q_predict = max_a Q(St,a)  if random() < exploration_rate  \n",
    "            Execute action a           \n",
    "            R(S_t, a) = reward\n",
    "            S_t = S_t+1 # New state\n",
    "            save_in_memory = S_t, a, R(S_t, a), S_t+1, is_final_state?\n",
    "        batch = select actions from memory (randomly)\n",
    "        for each element in batch do:\n",
    "            Predict (Neuronal network) = Q_predict(S_t, a)            \n",
    "            Q_obj(S_t, a) =  R(S_t, a)  + (gamma . max_a'Q(S_t+1_, a'))     \n",
    "            Train Neuronal Netwok (X= S_t, Y = Q_obj(S_t, a), loss = MSE) \n",
    "                Loss => 1/2 Sum(TD(S_t,a)^2) # temporal difference error\n",
    "        end\n",
    "    end\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through episodes. Evolution prints will appear every 10 episodes.\n",
      "\tEPISODE 1 - Actions: 236 - Reward: -7238.0\n",
      "\tEPISODE 2 - Actions: 15 - Reward: -144.0\n",
      "\tEPISODE 3 - Actions: 49 - Reward: -1320.0\n",
      "\tEPISODE 4 - Actions: 90 - Reward: -2412.0\n",
      "\tEPISODE 5 - Actions: 45 - Reward: -1252.0\n",
      "\tEPISODE 6 - Actions: 22 - Reward: -732.0\n",
      "\tEPISODE 7 - Actions: 149 - Reward: -6680.0\n",
      "\tEPISODE 8 - Actions: 41 - Reward: -1724.0\n",
      "\tEPISODE 9 - Actions: 40 - Reward: -1716.0\n",
      "\tEPISODE 10 - Actions: 155 - Reward: -6272.0\n",
      "CTRL POINT: 1\n",
      "New expl.ratio: 0.47. Best reward now: -144.0. Best n_step now: 15\n",
      "\tEPISODE 11 - Actions: 14 - Reward: -124.0\n",
      "\tEPISODE 12 - Actions: 91 - Reward: -5884.0\n",
      "\tEPISODE 13 - Actions: 47 - Reward: -1504.0\n",
      "\tEPISODE 14 - Actions: 17 - Reward: -548.0\n",
      "\tEPISODE 15 - Actions: 186 - Reward: -6892.0\n",
      "\tEPISODE 16 - Actions: 159 - Reward: -6880.0\n",
      "\tEPISODE 17 - Actions: 14 - Reward: -244.0\n",
      "\tEPISODE 18 - Actions: 78 - Reward: -2160.0\n",
      "\tEPISODE 19 - Actions: 29 - Reward: -804.0\n",
      "\tEPISODE 20 - Actions: 20 - Reward: -20.0\n",
      "CTRL POINT: 2\n",
      "New expl.ratio: 0.63. Best reward now: -20.0. Best n_step now: 20\n",
      "\tEPISODE 21 - Actions: 28 - Reward: -1508.0\n",
      "\tEPISODE 22 - Actions: 107 - Reward: -1504.0\n",
      "\tEPISODE 23 - Actions: 64 - Reward: -1000.0\n",
      "\tEPISODE 24 - Actions: 105 - Reward: -1800.0\n",
      "\tEPISODE 25 - Actions: 160 - Reward: -1792.0\n",
      "\tEPISODE 26 - Actions: 16 - Reward: -12.0\n",
      "\tEPISODE 27 - Actions: 18 - Reward: -312.0\n",
      "\tEPISODE 28 - Actions: 41 - Reward: -360.0\n",
      "\tEPISODE 29 - Actions: 83 - Reward: -744.0\n",
      "\tEPISODE 30 - Actions: 24 - Reward: 92.0\n",
      "CTRL POINT: 3\n",
      "New expl.ratio: 0.71. Best reward now: 92.0. Best n_step now: 24\n",
      "\tEPISODE 31 - Actions: 398 - Reward: -2628.0\n",
      "\tEPISODE 32 - Actions: 34 - Reward: -228.0\n",
      "\tEPISODE 33 - Actions: 123 - Reward: -1184.0\n",
      "\tEPISODE 34 - Actions: 47 - Reward: -788.0\n",
      "\tEPISODE 35 - Actions: 95 - Reward: -2020.0\n",
      "\tEPISODE 36 - Actions: 12 - Reward: -64.0\n",
      "\tEPISODE 37 - Actions: 64 - Reward: 12.0\n",
      "\tEPISODE 38 - Actions: 20 - Reward: 40.0\n",
      "\tEPISODE 39 - Actions: 157 - Reward: -952.0\n",
      "\tEPISODE 40 - Actions: 39 - Reward: -116.0\n",
      "CTRL POINT: 4\n",
      "New expl.ratio: 0.76. Best reward now: 92.0. Best n_step now: 24\n",
      "\tEPISODE 41 - Actions: 141 - Reward: -556.0\n",
      "\tEPISODE 42 - Actions: 40 - Reward: 0.0\n",
      "\tEPISODE 43 - Actions: 188 - Reward: -416.0\n",
      "\tEPISODE 44 - Actions: 40 - Reward: 60.0\n",
      "\tEPISODE 45 - Actions: 52 - Reward: 36.0\n",
      "\tEPISODE 46 - Actions: 152 - Reward: -404.0\n",
      "\tEPISODE 47 - Actions: 44 - Reward: 52.0\n",
      "\tEPISODE 48 - Actions: 70 - Reward: -416.0\n",
      "\tEPISODE 49 - Actions: 28 - Reward: 144.0\n",
      "\tEPISODE 50 - Actions: 12 - Reward: 56.0\n",
      "CTRL POINT: 5\n",
      "New expl.ratio: 0.79. Best reward now: 144.0. Best n_step now: 28\n",
      "\tEPISODE 51 - Actions: 17 - Reward: -132.0\n",
      "\tEPISODE 52 - Actions: 10 - Reward: 180.0\n",
      "\tEPISODE 53 - Actions: 18 - Reward: -780.0\n",
      "\tEPISODE 54 - Actions: 8 - Reward: 124.0\n",
      "\tEPISODE 55 - Actions: 36 - Reward: 68.0\n",
      "\tEPISODE 56 - Actions: 12 - Reward: 116.0\n",
      "\tEPISODE 57 - Actions: 7 - Reward: 68.0\n",
      "\tEPISODE 58 - Actions: 6 - Reward: 188.0\n",
      "\tEPISODE 59 - Actions: 9 - Reward: 4.0\n",
      "\tEPISODE 60 - Actions: 55 - Reward: -4336.0\n",
      "CTRL POINT: 6\n",
      "New expl.ratio: 0.81. Best reward now: 188.0. Best n_step now: 6\n",
      "\tEPISODE 61 - Actions: 124 - Reward: -3692.0\n",
      "\tEPISODE 62 - Actions: 221 - Reward: -1020.0\n",
      "\tEPISODE 63 - Actions: 78 - Reward: -372.0\n",
      "\tEPISODE 64 - Actions: 61 - Reward: -576.0\n",
      "\tEPISODE 65 - Actions: 450 - Reward: -2196.0\n",
      "\tEPISODE 66 - Actions: 148 - Reward: -1052.0\n",
      "\tEPISODE 67 - Actions: 65 - Reward: -288.0\n",
      "\tEPISODE 68 - Actions: 267 - Reward: -1352.0\n",
      "\tEPISODE 69 - Actions: 97 - Reward: -592.0\n",
      "\tEPISODE 70 - Actions: 19 - Reward: -76.0\n",
      "CTRL POINT: 7\n",
      "New expl.ratio: 0.83. Best reward now: 188.0. Best n_step now: 6\n",
      "\tEPISODE 71 - Actions: 251 - Reward: -960.0\n",
      "\tEPISODE 72 - Actions: 245 - Reward: -5792.0\n",
      "\tEPISODE 73 - Actions: 36 - Reward: -172.0\n",
      "\tEPISODE 74 - Actions: 61 - Reward: -220.0\n",
      "\tEPISODE 75 - Actions: 111 - Reward: -500.0\n",
      "\tEPISODE 76 - Actions: 64 - Reward: -108.0\n",
      "\tEPISODE 77 - Actions: 86 - Reward: -332.0\n",
      "\tEPISODE 78 - Actions: 89 - Reward: -336.0\n",
      "\tEPISODE 79 - Actions: 148 - Reward: -216.0\n",
      "\tEPISODE 80 - Actions: 234 - Reward: -984.0\n",
      "CTRL POINT: 8\n",
      "New expl.ratio: 0.84. Best reward now: 188.0. Best n_step now: 6\n",
      "\tEPISODE 81 - Actions: 152 - Reward: -224.0\n",
      "\tEPISODE 82 - Actions: 36 - Reward: 128.0\n",
      "\tEPISODE 83 - Actions: 82 - Reward: -380.0\n",
      "\tEPISODE 84 - Actions: 48 - Reward: 104.0\n",
      "\tEPISODE 85 - Actions: 92 - Reward: -104.0\n",
      "\tEPISODE 86 - Actions: 20 - Reward: 160.0\n",
      "\tEPISODE 87 - Actions: 170 - Reward: -260.0\n",
      "\tEPISODE 88 - Actions: 144 - Reward: -208.0\n",
      "\tEPISODE 89 - Actions: 18 - Reward: 104.0\n",
      "\tEPISODE 90 - Actions: 10 - Reward: 120.0\n",
      "CTRL POINT: 9\n",
      "New expl.ratio: 0.85. Best reward now: 188.0. Best n_step now: 6\n",
      "\tEPISODE 91 - Actions: 80 - Reward: -80.0\n",
      "\tEPISODE 92 - Actions: 63 - Reward: -104.0\n",
      "\tEPISODE 93 - Actions: 44 - Reward: -8.0\n",
      "\tEPISODE 94 - Actions: 18 - Reward: 44.0\n",
      "\tEPISODE 95 - Actions: 59 - Reward: -272.0\n",
      "\tEPISODE 96 - Actions: 8 - Reward: 184.0\n",
      "\tEPISODE 97 - Actions: 6 - Reward: 188.0\n",
      "\tEPISODE 98 - Actions: 6 - Reward: 128.0\n",
      "\tEPISODE 99 - Actions: 7 - Reward: 68.0\n",
      "\tEPISODE 100 - Actions: 6 - Reward: 188.0\n",
      "CTRL POINT: 10\n",
      "New expl.ratio: 0.86. Best reward now: 188.0. Best n_step now: 6\n",
      "\n",
      "Q_TABLE:\n",
      "State        Up       Down       Left     Right\n",
      "x0,y0 -4.185391  -0.601334  -5.494905  0.637828\n",
      "x0,y1 -3.590681   1.173275  -4.711586  0.668628\n",
      "x0,y2 -3.749042   2.737977  -4.834236  1.031872\n",
      "x0,y3 -4.100316   4.120616  -5.322391  1.396383\n",
      "x0,y4 -4.559403   5.322006  -6.052077  1.736980\n",
      "x1,y0 -3.569897  -4.181789  -9.477991 -0.051660\n",
      "x1,y1 -2.738188  -2.433571  -8.602196 -0.035814\n",
      "x1,y2 -2.081183  -0.672698  -7.825315  0.048654\n",
      "x1,y3 -2.234618   0.902669  -7.894341  0.474702\n",
      "x1,y4 -2.432147   2.457999  -8.055221  0.861938\n",
      "x2,y0 -4.242814  -6.869555 -14.073687 -0.515891\n",
      "x2,y1 -2.664366  -5.825153 -12.956524 -0.887860\n",
      "x2,y2 -1.669383  -4.174804 -11.993375 -0.812369\n",
      "x2,y3 -0.976189  -2.501647 -11.346458 -0.639576\n",
      "x2,y4 -1.036608  -0.935567 -11.326475 -0.189654\n",
      "x3,y0 -5.276457  -9.352361 -18.801199 -0.825845\n",
      "x3,y1 -3.666439  -8.389171 -17.691885 -1.235957\n",
      "x3,y2 -2.087259  -7.270653 -16.535715 -1.553754\n",
      "x3,y3 -0.737440  -5.846934 -15.409179 -1.651811\n",
      "x3,y4 -0.146988  -4.206331 -14.904435 -1.428201\n",
      "x4,y0 -6.392763 -11.795436 -23.612743 -1.169633\n",
      "x4,y1 -4.782745 -10.832248 -22.503431 -1.579745\n",
      "x4,y2 -3.172726  -9.869058 -21.394117 -1.989856\n",
      "x4,y3 -1.610159  -8.666872 -20.212704 -2.257929\n",
      "x4,y4 -0.072808  -7.357931 -19.002626 -2.460409\n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "          y0        y1        y2        y3        y4\n",
      "x0  0.637828  1.173275  2.737977  4.120616  5.322006\n",
      "x1 -0.051660 -0.035814  0.048654  0.902669  2.457999\n",
      "x2 -0.515891 -0.887860 -0.812369 -0.639576 -0.189654\n",
      "x3 -0.825845 -1.235957 -1.553754 -0.737440 -0.146988\n",
      "x4 -1.169633 -1.579745 -1.989856 -1.610159 -0.072808\n",
      "\n",
      "BEST ACTIONS:\n",
      "       y0     y1     y2     y3     y4\n",
      "x0  Right   Down   Down   Down   Down\n",
      "x1  Right  Right  Right   Down   Down\n",
      "x2  Right  Right  Right  Right  Right\n",
      "x3  Right  Right  Right     Up     Up\n",
      "x4  Right  Right  Right     Up     Up\n",
      "\n",
      "STEPS: \n",
      "   [[4, 1], [4, 2], [4, 3], [3, 3], [2, 3], [2, 4]]\n",
      "\n",
      "PATH:\n",
      "   y0 y1 y2 y3 y4\n",
      "x0  -  -  -  -  -\n",
      "x1  -  -  -  -  -\n",
      "x2  -  -  -  5  6\n",
      "x3  -  -  -  4  -\n",
      "x4  0  1  2  3  -\n"
     ]
    }
   ],
   "source": [
    "# Apply agent deep q-learning and print results\n",
    "dqae.agent_deep_q_learning(learner=dqlearn.DeepQLearner,                           \n",
    "                           environment = deepcopy(environment),  # Agent same environment.\n",
    "                           num_episodes=100, # Note: adjust if grid is greater.\n",
    "                           # Near 0: move to the next most rewarding state. Near 1:with a view to maximizing the final reward\n",
    "                           discount_factor=0.1, \n",
    "                           # Aim to obtain 95% explotation, during the learning process is adjusted.\n",
    "                           # Starts from exploration and gradually goes to explotation.\n",
    "                           explotation_ratio=0.95, # Greedy control\n",
    "                           max_steps= 500, \n",
    "                           verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
