{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCEMENT LEARNING WITH DEEP Q-LEARNING\n",
    "\n",
    "## DEEP Q-LEARNING OVERVIEW\n",
    "\n",
    "The logic of the Deep _Q-learning algorithm_ is very similar to _Q-learning_, but in this case, the policy or _Q-table_ is not learned directly through the _Q-function_. Instead, it is learned by a neural network.\n",
    "\n",
    "A neural network is defined such that, given a state, it returns the best Q(s, a) value across all possible actions, selecting only those with the highest Q(s, a) values. The network consists of:\n",
    "\n",
    "- **Input Layer**: Represents the necessary neurons to encode the agent's current state (In 2D grid example, x and y coordinates, so 2 neurons).\n",
    "- **Output Layer**: Encodes the actions the agent can take in the environment (In 2D grid example, 4 actions: up, down, left, and right).\n",
    "- **Intermediate Layers**: Determined based on the complexity of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Add the parent directory (where \"modules\" is located) to the Python path\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from modules import environment_creation_functions as ecf\n",
    "from modules import environment as env\n",
    "from modules import learners as learn\n",
    "from modules import environment_agent_execution as eae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD ENVIRONMENT\n",
    "\n",
    "Initial state of the agent is represented in green color, the final or destination state in red and barrier in black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment\n",
    "dim, grid_matrix_positions, barriers, src, dest, reward_matrix  = ecf.create_environment(False, 20, 100)\n",
    "environment = env.Environment(reward_matrix, list(src), list(dest)) # convert states from tuple into list\n",
    "\n",
    "# visualize created environment\n",
    "%matplotlib inline\n",
    "ecf.visualize_environment(dim, barriers, src, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEEP Q-LEARNING ALGORITHM\n",
    "\n",
    "### LEARNING PROCESS\n",
    "\n",
    "The learning process in the Deep Q-learning algorithm does not occur per action, as it does in the `Q-learning` or `SARSA` algorithms. Instead, it occurs over entire episodes, where the algorithm takes actions and stores them in memory. This memory is called `Experience Replay`, and for each action, it records:\n",
    "\n",
    "- the **current state**,\n",
    "- the **action taken**,\n",
    "- the **reward** obtained,\n",
    "- the **next state**, and\n",
    "- whether or not **it is a terminal state**.\n",
    "\n",
    "Once the memory is filled, a set of actions is selected (i.e.: randomly), and a model is trained to adjust the weights, allowing the network to learn from each action.\n",
    "\n",
    "- Algorithm pseudocode:\n",
    "\n",
    "   ```\n",
    "   # Initialize Memory (Experience Replay) : Current State | Taken Action | Reward | Next State | Is final state?\n",
    "   # Set memory size i.e. mem_size = 200\n",
    "   # Neuronal Network initialization\n",
    "   For each episode do:\n",
    "        s = initial state\n",
    "        actions_per_episode = 0\n",
    "        memory = empty\n",
    "        while (s != not final) and (actions_per_episode < mem_size) do:\n",
    "            a = Choose action. Cases:\n",
    "                - random(action) if random() >= exploration_rate \n",
    "                - Predict (Neuronal network): Q_predict = max_a Q(St,a)  if random() < exploration_rate  \n",
    "            Execute action a           \n",
    "            R(S_t, a) = reward\n",
    "            S_t = S_t+1 # New state\n",
    "            save_in_memory = S_t, a, R(S_t, a), S_t+1, is_final_state?\n",
    "        batch = select actions from memory (randomly)\n",
    "        for each element in batch do:\n",
    "            Predict (Neuronal network) = Q_predict(S_t, a)            \n",
    "            Q_obj(S_t, a) =  R(S_t, a)  + (gamma . max_a'Q(S_t+1_, a'))     \n",
    "            Train Neuronal Netwok (X= S_t, Y = Q_obj(S_t, a), loss = MSE) \n",
    "                Loss => 1/2 Sum(TD(S_t,a)^2) # temporal difference error\n",
    "        end\n",
    "    end\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHORT-TERM FOCUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LONG-TERM FOCUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
