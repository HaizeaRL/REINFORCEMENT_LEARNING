{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCEMENT LEARNING WITH Q-LEARNING AND SARSA\n",
    "\n",
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Add the parent directory (where \"modules\" is located) to the Python path\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from modules import environment_creation_functions as ecf\n",
    "from modules import environment as env\n",
    "from modules import learners as learn\n",
    "from modules import environment_agent_execution as eae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD ENVIRONMENT\n",
    "\n",
    "Initial state of the agent is represented in green color, the final or destination state in red and barrier in black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating maze\n",
      "Grid of: 9 x 9\n",
      "Barriers count: 20\n",
      "Source: (3, 0)\n",
      "Destination: (3, 8)\n",
      "Path to destination found.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAFICAYAAAA24bcOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAJRUlEQVR4nO3bMY7k1hmF0Z9GQwDFBTBXbHhiQnHtSMtwqtVwF4I3UQugCCgwHQxGqOmW+5Jt1Lw3xjmRA8LvimR9Gg6g4TiOowD4r/7WegBA74QSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIXs5eOAzDM3cAfHNn/8PE06Gsqrrf7zVN04cG/a+2bat5npvveL2lF+u61rIszc7v5fn0sqOnLb3s6GnL1d/wpVBO09T0Jve2oyfjOHZzT3p5Pr3sqOpnSy87qvrakvg7SoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqA4OXKxdu2PWvHpbNb7ujh/L+y77vn09GO1+e7J2/P7+WenDEcx3GcunAYPjQIoFcn8+fTGyC59Ondi3Vda1mWZudv21bzPP+5ZRzHJjv2fa/b7VZVVff7vaZparKj6ut70nJLLzteb+Gtlr/jq8/muwzlOI5NfwCPlmVpGoUvpmnq5p70sqWXHfy1nn7HiU9vgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoDgpfWAj9j3vbZta3b+49l2vD3fPenj/N61/B1fPXc4juM4deEwfGgQQK9O5s+nN0By6dN7Xdcax/FZW96173vdbreqqrrf7zVNU5MdVZ//2D7Pc/Mtjzt6sq5rLcvS5Oxe70kvfv311/r06VOz8x9/x9+TS6FclqVpFL6YpqlpKB/1tKUX4zi6J5369OlT/fzzz83O/17/3tanN0AglACBUAIEQgkQCCVAIJQAgVACBEIJEAglQCCUAIFQAgRCCRAIJUAglACBUAIEQgkQCCVAIJQAgVACBEIJEAglQCCUAIFQAgRCCRAIJUAglACBUAIEQgkQCCVAIJQAwcuVi7dte9aOS2e33PH6/F7uSU/2fW+2rdd70ouWz6bq+30+w3Ecx6kLh+HZWwC+qZP58+kNkFz69K5fquqH5wyJ/qiqf37+n+tvay0/LY2GfP58mOe5qqru93tN09R8R0/Wda1lafN8Hu/Juq41jmOTHVWfP3Nvt1vzLY87Wr6vVa9+O1XVaslWVVd+OddC+UO1C+WD8cex6cN+NE1TN1t6MY59PJ9lWZpHoYctjzt6el+nahfKq3x6AwRCCRAIJUAglACBUAIEQgkQCCVAIJQAgVACBEIJEAglQCCUAIFQAgRCCRAIJUAglACBUAIEQgkQCCVAIJQAgVACBEIJEAglQCCUAIFQAgRCCRAIJUAglACBUAIEL5eu/uNJKy6evf++17ZtzaY8nt3Ljp7se7vn08uzeX1+L+9JV/ek5Y6L1w/HcRynLhyGD8wB6NfJ/Pn0BkgufXqvv601/jg+a8u79t/3uv3jVlVV9/u9pmlqsqPq8+fDPM/Nzu/duq61LEuTsx+fTU/vScstvex4veXv1e5Pav+uqn9duP5SKJeflqYP+4tpmpo+bN43jmMXz6en96SXLb3sqLoWqtZ8egMEQgkQCCVAIJQAgVACBEIJEAglQCCUAIFQAgRCCRAIJUAglACBUAIEQgkQCCVAIJQAgVACBEIJEAglQCCUAIFQAgRCCRAIJUAglACBUAIEQgkQCCVAIJQAgVACBC9XLt627Vk7Lp3dckcP5/du3/dm96jX98Rvp4/zP2o4juM4deEwPHsLwDd1Mn8+vQGSS5/e9/u9pml61pZ3bdtW8zw33/F6C2+t61rLsjQ5u9f3xG/n7ZZ1XWscxyY79n2v2+12+vpLoZymqelN7m0Hf20cxy6eT0/vSS9betlRVbUsS9N/eVzh0xsgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIHi5cvG2bc/acensljt6OL93+743u0e9vid+O2/P7+WenDEcx3GcunAYPjQIoFcn8+fTGyC59Ondi3Vda1mWZudv21bzPP+5ZRzHJjv2fa/b7dZ8x+st9/u9pmlqsuPx2fSk5Tv7eE9aPpuetlx9T77LUI7j2PRhP1qWpenD7mHH6y3TNHXzfHrRyzvb07PpaUvi0xsgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgOCl9YCP2Pe9tm1rdv7j2Xa8Pb+Xe9KTlu9sL8/m9fnf03syHMdxnLpwGD40CKBXJ/Pn0xsgufTpva5rjeP4rC3v2ve9brdbk7Pfs65rLcvS5Oxt22qe56qqut/vNU1Tkx09bXnc0fJ9rfr6ne3lnnhP3u4441Iol2Vp+g/Wo3Ecm754X0zT1MWOqn62tHxfq75+Z3u5J73sqOprS+LTGyAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgEEqAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoBAKAECoQQIhBIgeLly8bZtz9rR9dnv2fe92bbHc1vfn1629LLj9fnuydvze7knZwzHcRynLhyGDw0C6NXJ/Pn0BkgufXrf7/eapulZW961bVvN81xVVeu61jiOTXZUff7cvt1uVdXPPWm5o6ctjzt4a13XWpal2fnf6/O5FMppmpr+GL9YlqV5FL7o5Z70sqOqry18bRxHz+YDfHoDBEIJEAglQCCUAIFQAgRCCRAIJUAglACBUAIEQgkQCCVAIJQAgVACBEIJEAglQCCUAIFQAgRCCRAIJUAglACBUAIEQgkQCCVAIJQAgVACBEIJEAglQCCUAIFQAgQvVy7etu1ZOy6d3XLH6/Pdk7fn93JPeGvfd8/nA4bjOI5TFw7Ds7cAfFMn83f+T5Rn/w8B/t/4O0qAQCgBAqEECIQSIBBKgEAoAQKhBAiEEiAQSoDgP02UaTdaVK6MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create environment\n",
    "dim, grid_matrix_positions, barriers, src, dest, reward_matrix  = ecf.create_environment(False, 4, 20)\n",
    "environment = env.Environment(reward_matrix, list(src), list(dest)) # convert states from tuple into list\n",
    "\n",
    "# visualize created environment\n",
    "%matplotlib inline\n",
    "ecf.visualize_environment(dim, barriers, src, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCEMENT LEARNING RULES\n",
    "\n",
    "- **Penalty for Each Step**: -1\n",
    "- **Penalty for Hitting a Barrier** (Black Boxes): -30\n",
    "- **Penalty for Moving Out of Bounds**: -50\n",
    "\n",
    "### Q-LEARNER ALGORITHM\n",
    "\n",
    "- Algorithm pseudocode:\n",
    "\n",
    "   ```\n",
    "   Q(,) = 0 # Policy Initialization\n",
    "   For each episode do:\n",
    "        s = initial state\n",
    "        For each step, state s (not final) do:\n",
    "            a = Choose action. Cases:\n",
    "                - random(action) if random() <= exploration_rate \n",
    "                - max_a Q(s,)  if random() > exploration_rate  \n",
    "            Execute action a\n",
    "            s = s' # New state\n",
    "            R(s, a) = reward\n",
    "            Q(s, a)' = Q(s, a) + alpha[R(s, a) + (gamma . max_a'Q(s', a')) - Q(s, a)]       \n",
    "        end\n",
    "    end\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHORT-TERM FOCUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Actions: 282 - Reward: -2872.0\n",
      "EPISODE 2 - Actions: 56 - Reward: -422.0\n",
      "EPISODE 3 - Actions: 178 - Reward: -522.0\n",
      "EPISODE 4 - Actions: 150 - Reward: -464.0\n",
      "EPISODE 5 - Actions: 90 - Reward: 10.0\n",
      "EPISODE 6 - Actions: 225 - Reward: -234.0\n",
      "EPISODE 7 - Actions: 197 - Reward: -176.0\n",
      "EPISODE 8 - Actions: 197 - Reward: -206.0\n",
      "EPISODE 9 - Actions: 122 - Reward: -210.0\n",
      "EPISODE 10 - Actions: 289 - Reward: -366.0\n",
      "EPISODE 11 - Actions: 152 - Reward: -172.0\n",
      "EPISODE 12 - Actions: 178 - Reward: -78.0\n",
      "EPISODE 13 - Actions: 142 - Reward: -42.0\n",
      "EPISODE 14 - Actions: 152 - Reward: -82.0\n",
      "EPISODE 15 - Actions: 334 - Reward: -234.0\n",
      "EPISODE 16 - Actions: 117 - Reward: -156.0\n",
      "EPISODE 17 - Actions: 204 - Reward: -164.0\n",
      "EPISODE 18 - Actions: 240 - Reward: -170.0\n",
      "EPISODE 19 - Actions: 66 - Reward: 34.0\n",
      "EPISODE 20 - Actions: 294 - Reward: -254.0\n",
      "EPISODE 21 - Actions: 46 - Reward: 54.0\n",
      "EPISODE 22 - Actions: 184 - Reward: -144.0\n",
      "EPISODE 23 - Actions: 205 - Reward: -184.0\n",
      "EPISODE 24 - Actions: 46 - Reward: 54.0\n",
      "EPISODE 25 - Actions: 120 - Reward: -50.0\n",
      "EPISODE 26 - Actions: 134 - Reward: -124.0\n",
      "EPISODE 27 - Actions: 132 - Reward: -92.0\n",
      "EPISODE 28 - Actions: 92 - Reward: -22.0\n",
      "EPISODE 29 - Actions: 92 - Reward: -82.0\n",
      "EPISODE 30 - Actions: 160 - Reward: -188.0\n",
      "\n",
      "BEST EPISODE:\n",
      "EPISODE 24\n",
      "\tActions: 46\n",
      "\tReward: 54.0\n",
      "\n",
      "Q_TABLE:\n",
      "State         Up      Down      Left      Right\n",
      "x0,y0  -5.000000 -3.100000 -5.000000  -1.091844\n",
      "x0,y1  -9.505375 -1.090492 -1.090094  -8.402900\n",
      "x0,y2  -5.000000 -0.288814 -0.198886  -0.281711\n",
      "x0,y3 -13.564273 -1.058086 -5.891910  -1.054437\n",
      "x0,y4 -13.556775 -1.043825 -1.041805  -1.045365\n",
      "x0,y5  -5.000000 -1.045033 -1.039733  -1.039214\n",
      "x0,y6  -5.000000 -5.891010 -1.038881  -1.031855\n",
      "x0,y7  -5.000000 -3.100000 -1.031895  -1.031795\n",
      "x0,y8  -9.506352 -1.031828 -1.031879  -5.000000\n",
      "x1,y0  -0.200703 -0.101000 -5.000000  -0.198116\n",
      "x1,y1  -1.089457 -1.091168 -5.891000  -1.090420\n",
      "x1,y2  -8.403878 -1.084497 -1.084507  -1.083469\n",
      "x1,y3  -1.064978 -8.403819 -1.060923  -1.064842\n",
      "x1,y4  -1.033903 -1.040032 -1.037440  -1.038942\n",
      "x1,y5  -1.039228 -3.100000 -1.044318  -5.891000\n",
      "x1,y6  -0.200059 -3.101000 -0.201205  -3.100000\n",
      "x1,y7  -0.100000 -0.100000 -3.100000  -0.100000\n",
      "x1,y8  -1.031935 -3.100000 -3.100000  -5.000000\n",
      "x2,y0  -5.890000 -1.100337 -9.507904  -1.100048\n",
      "x2,y1  -1.095249 -1.095762 -1.096362  -1.095704\n",
      "x2,y2  -1.092025 -1.092637 -1.090952 -10.664558\n",
      "x2,y3  -0.282724 -5.891000 -0.200467  -0.285002\n",
      "x2,y4  -0.969400 -0.980486 -5.891000  -3.100000\n",
      "x2,y5  -0.100000 -0.100000  0.000000  -5.890000\n",
      "x2,y6  -3.100000 -0.100000 -3.100000  -0.190000\n",
      "x2,y7  -3.100000 -0.100000 -3.100000  -3.100000\n",
      "x2,y8  -0.100000  9.900000  0.000000  -5.000000\n",
      "x3,y0  -1.103258 -1.103401 -9.500000  -1.103373\n",
      "x3,y1  -1.098468 -1.098401 -1.098388  -1.098304\n",
      "x3,y2  -1.095759 -5.891000 -1.095951  -5.891000\n",
      "x3,y3  -3.100000 -3.100000 -0.199246  -0.191000\n",
      "x3,y4  -0.826400 -0.835726 -3.100000  -0.824393\n",
      "x3,y5  -3.100000 -0.589541 -0.546278  -0.551884\n",
      "x3,y6  -3.100000 -0.190000 -0.101000   3.812446\n",
      "x3,y7  -0.100000 -0.100000 -0.182204  86.963911\n",
      "x3,y8   0.000000  0.000000  0.000000   0.000000\n",
      "x4,y0  -1.098329 -1.098733 -5.000000  -1.098325\n",
      "x4,y1  -1.096079 -1.095987 -1.095849  -3.100000\n",
      "x4,y2  -0.198136 -3.100000 -0.101000  -3.100000\n",
      "x4,y3  -3.100000 -3.100000 -3.100000  -0.100000\n",
      "x4,y4  -0.757531 -0.787163 -3.100000  -0.776625\n",
      "x4,y5  -0.634128 -0.637568 -0.642101  -0.626163\n",
      "x4,y6  -0.368974 -0.421029 -0.423184  -0.396615\n",
      "x4,y7   3.141534 -0.191000 -0.101900  -5.701900\n",
      "x4,y8  26.829000  0.000000  0.000000  -5.000000\n",
      "x5,y0  -1.093452 -8.402900 -5.000000  -1.094090\n",
      "x5,y1  -1.089624 -1.089520 -1.089018 -10.665405\n",
      "x5,y2  -3.100000 -0.284792 -0.364905  -3.100000\n",
      "x5,y3  -3.100000 -0.100000 -3.100000  -0.101000\n",
      "x5,y4  -0.811832 -0.824480 -3.100000  -0.831856\n",
      "x5,y5  -0.676048 -3.100000 -0.685730  -0.711164\n",
      "x5,y6  -0.582620 -0.590675 -0.540810  -0.586538\n",
      "x5,y7  -0.507010 -0.481905 -0.482007  -0.537005\n",
      "x5,y8  -3.001000 -0.539769 -0.587337  -5.000000\n",
      "x6,y0  -0.103514 -0.196395 -5.000000  -0.196691\n",
      "x6,y1  -1.070769 -8.402900 -3.100000  -1.072864\n",
      "x6,y2  -8.404729 -1.034671 -1.039447  -1.038813\n",
      "x6,y3  -3.100000 -0.994594 -0.991411  -0.998046\n",
      "x6,y4  -0.931639 -0.943239 -0.933264  -3.100000\n",
      "x6,y5  -0.100000 -0.100000 -0.100000  -0.100000\n",
      "x6,y6  -0.633124 -0.640567 -3.100000  -0.674346\n",
      "x6,y7  -0.630429 -0.592112 -0.590554  -0.632655\n",
      "x6,y8  -0.631661 -0.592133 -0.590559  -5.000000\n",
      "x7,y0  -3.100000 -1.016298 -5.000000  -5.891000\n",
      "x7,y1  -0.191000 -0.191900 -0.100000  -0.198643\n",
      "x7,y2  -1.019747 -1.014036 -3.100000  -1.011715\n",
      "x7,y3  -0.985178 -0.983162 -0.988253  -0.990452\n",
      "x7,y4  -0.941555 -0.943599 -0.947351  -0.950360\n",
      "x7,y5  -3.100000 -3.100000 -0.871314  -0.876312\n",
      "x7,y6  -0.747627 -3.100000 -0.727748  -0.746501\n",
      "x7,y7  -0.676425 -0.635157 -0.640026  -0.673904\n",
      "x7,y8  -0.635051 -3.100000 -0.636999  -5.000000\n",
      "x8,y0  -1.014844 -5.000000 -5.000000  -1.016281\n",
      "x8,y1  -3.100000 -5.000000 -1.014445  -1.014445\n",
      "x8,y2  -1.007843 -9.508380 -1.006858  -1.003134\n",
      "x8,y3  -0.984291 -9.509428 -0.986298  -0.980584\n",
      "x8,y4  -0.956255 -9.503499 -0.961611  -5.891000\n",
      "x8,y5  -0.191000 -5.000000 -0.100000  -3.100000\n",
      "x8,y6  -0.100000 -5.000000 -3.100000  -0.101000\n",
      "x8,y7  -0.635721 -5.000000 -3.100000  -3.100000\n",
      "x8,y8  -0.100000 -5.000000 -0.100000  -5.000000\n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "          y0        y1        y2        y3        y4        y5        y6  \\\n",
      "x0 -1.091844 -1.090094 -0.198886 -1.054437 -1.041805 -1.039214 -1.031855   \n",
      "x1 -0.101000 -1.089457 -1.083469 -1.060923 -1.033903 -1.039228 -0.200059   \n",
      "x2 -1.100048 -1.095249 -1.090952 -0.200467 -0.969400  0.000000 -0.100000   \n",
      "x3 -1.103258 -1.098304 -1.095759 -0.191000 -0.824393 -0.546278  3.812446   \n",
      "x4 -1.098325 -1.095849 -0.101000 -0.100000 -0.757531 -0.626163 -0.368974   \n",
      "x5 -1.093452 -1.089018 -0.284792 -0.100000 -0.811832 -0.676048 -0.540810   \n",
      "x6 -0.103514 -1.070769 -1.034671 -0.991411 -0.931639 -0.100000 -0.633124   \n",
      "x7 -1.016298 -0.100000 -1.011715 -0.983162 -0.941555 -0.871314 -0.727748   \n",
      "x8 -1.014844 -1.014445 -1.003134 -0.980584 -0.956255 -0.100000 -0.100000   \n",
      "\n",
      "           y7         y8  \n",
      "x0  -1.031795  -1.031828  \n",
      "x1  -0.100000  -1.031935  \n",
      "x2  -0.100000   9.900000  \n",
      "x3  86.963911   0.000000  \n",
      "x4   3.141534  26.829000  \n",
      "x5  -0.481905  -0.539769  \n",
      "x6  -0.590554  -0.590559  \n",
      "x7  -0.635157  -0.635051  \n",
      "x8  -0.635721  -0.100000  \n",
      "\n",
      "BEST ACTIONS:\n",
      "       y0     y1     y2     y3     y4     y5     y6     y7    y8\n",
      "x0  Right   Left   Left  Right   Left  Right  Right  Right  Down\n",
      "x1   Down     Up  Right   Left     Up     Up     Up     Up    Up\n",
      "x2  Right     Up   Left   Left     Up   Left   Down   Down  Down\n",
      "x3     Up  Right     Up  Right  Right   Left  Right  Right    Up\n",
      "x4  Right   Left   Left  Right     Up  Right     Up     Up    Up\n",
      "x5     Up   Left   Down   Down     Up     Up   Left   Down  Down\n",
      "x6     Up     Up   Down   Left     Up     Up     Up   Left  Left\n",
      "x7   Down   Left  Right   Down     Up   Left   Left   Down    Up\n",
      "x8     Up  Right  Right  Right     Up   Left     Up     Up    Up\n",
      "\n",
      "STEPS: \n",
      "   [[3, 1], [4, 1], [4, 0], [4, 1], [3, 1], [2, 1], [2, 0], [3, 0], [4, 0], [5, 0], [5, 1], [4, 1], [5, 1], [6, 1], [6, 2], [6, 3], [6, 4], [5, 4], [6, 4], [7, 4], [7, 3], [7, 4], [7, 5], [7, 6], [7, 7], [7, 8], [6, 8], [6, 7], [6, 6], [5, 6], [6, 6], [7, 6], [6, 6], [6, 7], [5, 7], [5, 8], [6, 8], [7, 8], [7, 7], [6, 7], [6, 8], [5, 8], [5, 7], [4, 7], [3, 7], [3, 8]]\n",
      "\n",
      "PATH:\n",
      "     y0      y1  y2  y3     y4  y5        y6        y7        y8\n",
      "x0    -       -   -   -      -   -         -         -         -\n",
      "x1    -       -   -   -      -   -         -         -         -\n",
      "x2    7       6   -   -      -   -         -         -         -\n",
      "x3  0,8     1,5   -   -      -   -         -        45        46\n",
      "x4  3,9  2,4,12   -   -      -   -         -        44         -\n",
      "x5   10   11,13   -   -     18   -        30     35,43     36,42\n",
      "x6    -      14  15  16  17,19   -  29,31,33  28,34,40  27,37,41\n",
      "x7    -       -   -  21  20,22  23     24,32     25,39     26,38\n",
      "x8    -       -   -   -      -   -         -         -         -\n"
     ]
    }
   ],
   "source": [
    "# apply agent learning\n",
    "episodes_list, best_episode, src = eae.agent_learning(learner=learn.QLearner, # Learning algorithm\n",
    "                                        environment = deepcopy(environment),  # Agent environment.\n",
    "                                        num_episodes=30,\n",
    "                                        learning_rate=0.1,\n",
    "                                        discount_factor=0.1,    # Near 0, learn to move to the next most rewarding state\n",
    "                                        ratio_exploration=0.05, # Greedy control: Explore 5% of time and explote %95 of time\n",
    "                                        verbose=True)\n",
    "\n",
    "# print learning process\n",
    "eae.print_process_info(best_episode=best_episode, start_point = src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LONG-TERM FOCUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Actions: 337 - Reward: -2330.0\n",
      "EPISODE 2 - Actions: 113 - Reward: -468.0\n",
      "EPISODE 3 - Actions: 125 - Reward: -202.0\n",
      "EPISODE 4 - Actions: 97 - Reward: -700.0\n",
      "EPISODE 5 - Actions: 112 - Reward: -12.0\n",
      "EPISODE 6 - Actions: 157 - Reward: -166.0\n",
      "EPISODE 7 - Actions: 124 - Reward: -152.0\n",
      "EPISODE 8 - Actions: 42 - Reward: 58.0\n",
      "EPISODE 9 - Actions: 60 - Reward: -50.0\n",
      "EPISODE 10 - Actions: 112 - Reward: -102.0\n",
      "EPISODE 11 - Actions: 50 - Reward: -10.0\n",
      "EPISODE 12 - Actions: 121 - Reward: -130.0\n",
      "EPISODE 13 - Actions: 81 - Reward: -90.0\n",
      "EPISODE 14 - Actions: 134 - Reward: -34.0\n",
      "EPISODE 15 - Actions: 38 - Reward: 62.0\n",
      "EPISODE 16 - Actions: 70 - Reward: 30.0\n",
      "EPISODE 17 - Actions: 73 - Reward: -22.0\n",
      "EPISODE 18 - Actions: 77 - Reward: -26.0\n",
      "EPISODE 19 - Actions: 110 - Reward: -100.0\n",
      "EPISODE 20 - Actions: 22 - Reward: 48.0\n",
      "EPISODE 21 - Actions: 41 - Reward: 10.0\n",
      "EPISODE 22 - Actions: 73 - Reward: -112.0\n",
      "EPISODE 23 - Actions: 86 - Reward: -144.0\n",
      "EPISODE 24 - Actions: 50 - Reward: 50.0\n",
      "EPISODE 25 - Actions: 46 - Reward: 24.0\n",
      "EPISODE 26 - Actions: 63 - Reward: -42.0\n",
      "EPISODE 27 - Actions: 20 - Reward: 80.0\n",
      "EPISODE 28 - Actions: 38 - Reward: 62.0\n",
      "EPISODE 29 - Actions: 40 - Reward: 60.0\n",
      "EPISODE 30 - Actions: 58 - Reward: 12.0\n",
      "\n",
      "BEST EPISODE:\n",
      "EPISODE 27\n",
      "\tActions: 20\n",
      "\tReward: 80.0\n",
      "\n",
      "Q_TABLE:\n",
      "State        Up       Down       Left     Right\n",
      "x0,y0 -5.000000  -3.100000  -9.578187 -2.230975\n",
      "x0,y1 -9.679751  -2.226708  -2.226734 -3.100000\n",
      "x0,y2 -5.000000  -0.100000  -0.368868 -0.254492\n",
      "x0,y3 -5.000000  -1.581723  -3.100000 -1.511616\n",
      "x0,y4 -9.611978  -1.354336  -1.428362 -1.397546\n",
      "x0,y5 -5.000000  -1.396668  -1.403376 -1.367860\n",
      "x0,y6 -5.000000  -3.100000  -1.455380 -1.383274\n",
      "x0,y7 -9.595507  -3.100000  -1.430459 -1.357174\n",
      "x0,y8 -5.000000  -1.322341  -1.402863 -5.000000\n",
      "x1,y0 -0.109000  -0.100000   0.000000 -0.100000\n",
      "x1,y1 -2.157050  -2.126417  -3.100000 -2.172364\n",
      "x1,y2 -8.418100  -1.950675  -1.982336 -2.001839\n",
      "x1,y3 -1.633336  -3.100000  -1.668705 -1.673155\n",
      "x1,y4 -1.254149  -1.157375  -1.325316 -1.266673\n",
      "x1,y5 -1.308041  -3.100000  -1.345920 -5.890000\n",
      "x1,y6 -0.291245  -3.100000  -0.100000 -3.100000\n",
      "x1,y7 -0.131533  -0.100000  -3.100000  0.000000\n",
      "x1,y8 -1.341840  -3.100000  -3.100000 -5.000000\n",
      "x2,y0 -3.100000  -2.412376 -13.727701 -2.379788\n",
      "x2,y1 -2.236817  -2.227476  -2.247939 -2.230015\n",
      "x2,y2 -2.159813  -2.135235  -2.102031 -3.100000\n",
      "x2,y3 -0.100000  -3.110612  -0.199000 -0.100000\n",
      "x2,y4 -0.785042   0.921934  -5.890000 -8.418100\n",
      "x2,y5 -0.109000  -0.013215  -0.207910 -3.100000\n",
      "x2,y6 -3.109000   3.914307  -3.100000  0.000000\n",
      "x2,y7  0.000000   0.791000   0.000000  0.000000\n",
      "x2,y8  0.000000   9.900000   0.000000 -5.000000\n",
      "x3,y0 -2.524500  -2.502710  -5.000000 -2.547219\n",
      "x3,y1 -2.329326  -2.291913  -2.310174 -2.333387\n",
      "x3,y2 -2.216704  -8.418100  -2.178231 -8.430056\n",
      "x3,y3 -3.100000  -3.100000  -0.277626 -0.248973\n",
      "x3,y4 -0.446495  -0.393238  -5.890000  8.820963\n",
      "x3,y5 -5.898504  -0.199810  -0.207910 27.007548\n",
      "x3,y6 -8.367055  -0.207910  -0.100000 57.610967\n",
      "x3,y7  0.000000  -0.100000   0.000000 91.892810\n",
      "x3,y8  0.000000   0.000000   0.000000  0.000000\n",
      "x4,y0 -2.353073  -2.314603  -9.526797 -2.319804\n",
      "x4,y1 -2.287297  -2.227771  -2.231039 -3.100000\n",
      "x4,y2 -0.297946  -3.100000  -0.373491 -3.100000\n",
      "x4,y3 -3.100000  -3.100000  -3.100000 -0.100000\n",
      "x4,y4  0.038463  -0.397595  -3.100000 -0.465409\n",
      "x4,y5 -0.307519  -0.313187  -0.406260 -0.377200\n",
      "x4,y6  1.855210  -0.305029  -0.297818 -0.288100\n",
      "x4,y7 27.202851  -0.207910  -0.199000 -3.100000\n",
      "x4,y8  9.900000   0.000000  -0.100000  0.000000\n",
      "x5,y0 -2.220180  -5.890000  -5.000000 -2.171890\n",
      "x5,y1 -2.077194  -2.027587  -2.102617 -8.446784\n",
      "x5,y2 -3.100000  -0.531877  -0.435499 -3.100000\n",
      "x5,y3 -3.100000  -0.100000  -3.100000 -0.100000\n",
      "x5,y4 -0.639415  -0.787427  -3.100000 -0.690849\n",
      "x5,y5 -0.480795  -5.899000  -0.439282 -0.468251\n",
      "x5,y6 -0.385219  -0.399792  -0.417791 -0.366390\n",
      "x5,y7  3.974248  -0.297739  -0.297010 -0.199000\n",
      "x5,y8 -3.100000  -0.207910  -0.249528 -5.000000\n",
      "x6,y0 -0.100000  -0.300706  -5.000000 -0.100000\n",
      "x6,y1 -1.833044  -3.100000  -3.100000 -1.839427\n",
      "x6,y2 -8.415815  -1.572927  -1.680264 -1.525626\n",
      "x6,y3 -3.100000  -1.374699  -1.373724 -1.297425\n",
      "x6,y4 -1.075830  -1.061186  -1.163519 -3.100000\n",
      "x6,y5 -0.100000  -0.109000  -0.100000 -0.227477\n",
      "x6,y6 -0.490027  -0.523929  -3.100000 -0.544737\n",
      "x6,y7 -0.159569  -0.402635  -0.419286 -0.377200\n",
      "x6,y8 -0.369910  -0.305758  -0.305758 -5.000000\n",
      "x7,y0 -3.100000  -1.566224  -5.000000 -3.100000\n",
      "x7,y1 -0.109810  -0.109000  -0.109000 -0.100000\n",
      "x7,y2 -1.423174  -1.424826  -3.100000 -1.464695\n",
      "x7,y3 -1.333693  -1.305986  -1.307593 -1.346982\n",
      "x7,y4 -1.186156  -1.192136  -1.139660 -1.124033\n",
      "x7,y5 -3.100000  -5.899000  -0.872149 -0.902508\n",
      "x7,y6 -0.633005  -3.100000  -0.666927 -0.642168\n",
      "x7,y7 -0.567190  -0.490454  -0.423040 -0.466867\n",
      "x7,y8 -0.391780  -3.100000  -0.402790 -5.000000\n",
      "x8,y0 -1.507431  -5.000000  -5.000000 -1.543560\n",
      "x8,y1 -3.100000 -13.760605  -1.489129 -1.479349\n",
      "x8,y2 -1.439504  -9.535599  -1.404111 -1.436528\n",
      "x8,y3 -1.300651  -9.576638  -1.366230 -1.365534\n",
      "x8,y4 -1.314234  -5.000000  -1.273855 -3.100000\n",
      "x8,y5 -0.100000  -5.000000  -0.253302 -3.100000\n",
      "x8,y6 -0.100000  -5.000000  -3.100000 -0.225641\n",
      "x8,y7 -0.492461  -9.500000  -5.899000 -3.100000\n",
      "x8,y8 -0.100000  -5.000000  -0.100000  0.000000\n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "          y0        y1        y2        y3        y4         y5         y6  \\\n",
      "x0 -2.230975 -2.226708 -0.100000 -1.511616 -1.354336  -1.367860  -1.383274   \n",
      "x1  0.000000 -2.126417 -1.950675 -1.633336 -1.157375  -1.308041  -0.100000   \n",
      "x2 -2.379788 -2.227476 -2.102031 -0.100000  0.921934  -0.013215   3.914307   \n",
      "x3 -2.502710 -2.291913 -2.178231 -0.248973  8.820963  27.007548  57.610967   \n",
      "x4 -2.314603 -2.227771 -0.297946 -0.100000  0.038463  -0.307519   1.855210   \n",
      "x5 -2.171890 -2.027587 -0.435499 -0.100000 -0.639415  -0.439282  -0.366390   \n",
      "x6 -0.100000 -1.833044 -1.525626 -1.297425 -1.061186  -0.100000  -0.490027   \n",
      "x7 -1.566224 -0.100000 -1.423174 -1.305986 -1.124033  -0.872149  -0.633005   \n",
      "x8 -1.507431 -1.479349 -1.404111 -1.300651 -1.273855  -0.100000  -0.100000   \n",
      "\n",
      "           y7        y8  \n",
      "x0  -1.357174 -1.322341  \n",
      "x1   0.000000 -1.341840  \n",
      "x2   0.791000  9.900000  \n",
      "x3  91.892810  0.000000  \n",
      "x4  27.202851  9.900000  \n",
      "x5   3.974248 -0.207910  \n",
      "x6  -0.159569 -0.305758  \n",
      "x7  -0.423040 -0.391780  \n",
      "x8  -0.492461  0.000000  \n",
      "\n",
      "BEST ACTIONS:\n",
      "       y0     y1     y2     y3     y4     y5     y6     y7     y8\n",
      "x0  Right   Down   Down  Right   Down  Right  Right  Right   Down\n",
      "x1   Left   Down   Down     Up   Down     Up   Left  Right     Up\n",
      "x2  Right   Down   Left     Up   Down   Down   Down   Down   Down\n",
      "x3   Down   Down   Left  Right  Right  Right  Right  Right     Up\n",
      "x4   Down   Down     Up  Right     Up     Up     Up     Up     Up\n",
      "x5  Right   Down   Left   Down     Up   Left  Right     Up   Down\n",
      "x6     Up     Up  Right  Right   Down     Up     Up     Up   Down\n",
      "x7   Down  Right     Up   Down  Right   Left     Up   Left     Up\n",
      "x8     Up  Right   Left     Up   Left     Up     Up     Up  Right\n",
      "\n",
      "STEPS: \n",
      "   [[4, 0], [5, 0], [5, 1], [5, 0], [4, 0], [3, 0], [2, 0], [3, 0], [3, 1], [3, 2], [2, 2], [1, 2], [1, 3], [1, 4], [2, 4], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8]]\n",
      "\n",
      "PATH:\n",
      "       y0 y1  y2  y3  y4  y5  y6  y7  y8\n",
      "x0      -  -   -   -   -   -   -   -   -\n",
      "x1      -  -  12  13  14   -   -   -   -\n",
      "x2      7  -  11   -  15   -   -   -   -\n",
      "x3  0,6,8  9  10   -  16  17  18  19  20\n",
      "x4    1,5  -   -   -   -   -   -   -   -\n",
      "x5    2,4  3   -   -   -   -   -   -   -\n",
      "x6      -  -   -   -   -   -   -   -   -\n",
      "x7      -  -   -   -   -   -   -   -   -\n",
      "x8      -  -   -   -   -   -   -   -   -\n"
     ]
    }
   ],
   "source": [
    "# apply agent learning\n",
    "episodes_list, best_episode, src= eae.agent_learning(learner=learn.QLearner, # Learning algorithm\n",
    "                                        environment = deepcopy(environment), # Agent environment\n",
    "                                        num_episodes=30,\n",
    "                                        learning_rate=0.1,\n",
    "                                        discount_factor=0.9,    # Near 1, move with a view to maximizing the final reward\n",
    "                                        ratio_exploration=0.05, # Greedy control: Explore 5% of time and explote %95 of time\n",
    "                                        verbose=True)\n",
    "# print learning process\n",
    "eae.print_process_info(best_episode=best_episode, start_point = src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA ALGORITHM\n",
    "\n",
    "- Algorithm pseudocode:\n",
    "\n",
    "   ```\n",
    "   Q(,) = 0 # Policy Initialization\n",
    "   For each episode do:\n",
    "        s = initial state\n",
    "        For each step, state s (not final) do:\n",
    "            a = Choose action. Cases:\n",
    "                - random(action) if random() <= exploration_rate \n",
    "                - max_a Q(s,)  if random() > exploration_rate  \n",
    "            Execute action a\n",
    "            s = s' # New state\n",
    "            R(s, a) = reward\n",
    "            a' = Choose new action. Cases:\n",
    "                - random(action) if random() <= exploration_rate \n",
    "                - max_a' Q(s',)  if random() > exploration_rate   \n",
    "            Q(s, a)' = Q(s, a) + alpha[R(s, a) + (gamma . Q(s', a')) - Q(s, a)]       \n",
    "        end\n",
    "    end\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHORT-TERM FOCUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Actions: 118 - Reward: -1220.0\n",
      "EPISODE 2 - Actions: 355 - Reward: -2484.0\n",
      "EPISODE 3 - Actions: 232 - Reward: -380.0\n",
      "EPISODE 4 - Actions: 139 - Reward: -404.0\n",
      "EPISODE 5 - Actions: 244 - Reward: -302.0\n",
      "EPISODE 6 - Actions: 58 - Reward: 12.0\n",
      "EPISODE 7 - Actions: 238 - Reward: -168.0\n",
      "EPISODE 8 - Actions: 62 - Reward: 38.0\n",
      "EPISODE 9 - Actions: 340 - Reward: -564.0\n",
      "EPISODE 10 - Actions: 182 - Reward: -142.0\n",
      "EPISODE 11 - Actions: 228 - Reward: -188.0\n",
      "EPISODE 12 - Actions: 206 - Reward: -264.0\n",
      "EPISODE 13 - Actions: 203 - Reward: -280.0\n",
      "EPISODE 14 - Actions: 172 - Reward: -132.0\n",
      "EPISODE 15 - Actions: 100 - Reward: -98.0\n",
      "EPISODE 16 - Actions: 200 - Reward: -130.0\n",
      "EPISODE 17 - Actions: 26 - Reward: 74.0\n",
      "EPISODE 18 - Actions: 141 - Reward: -120.0\n",
      "EPISODE 19 - Actions: 162 - Reward: -152.0\n",
      "EPISODE 20 - Actions: 308 - Reward: -516.0\n",
      "EPISODE 21 - Actions: 167 - Reward: -146.0\n",
      "EPISODE 22 - Actions: 82 - Reward: 18.0\n",
      "EPISODE 23 - Actions: 78 - Reward: -38.0\n",
      "EPISODE 24 - Actions: 30 - Reward: 70.0\n",
      "EPISODE 25 - Actions: 68 - Reward: 32.0\n",
      "EPISODE 26 - Actions: 52 - Reward: 18.0\n",
      "EPISODE 27 - Actions: 88 - Reward: -18.0\n",
      "EPISODE 28 - Actions: 42 - Reward: 58.0\n",
      "EPISODE 29 - Actions: 88 - Reward: -18.0\n",
      "EPISODE 30 - Actions: 94 - Reward: -54.0\n",
      "\n",
      "BEST EPISODE:\n",
      "EPISODE 17\n",
      "\tActions: 26\n",
      "\tReward: 74.0\n",
      "\n",
      "Q_TABLE:\n",
      "State         Up      Down       Left      Right\n",
      "x0,y0  -9.501010 -3.100000  -9.506413  -1.044893\n",
      "x0,y1  -5.000000 -1.039400  -1.035993  -5.891000\n",
      "x0,y2  -5.000000 -0.197893  -0.100000  -0.195369\n",
      "x0,y3  -9.506788 -0.996307  -3.100000  -0.994151\n",
      "x0,y4  -5.000000 -0.982379  -0.973173  -0.971168\n",
      "x0,y5 -13.561335 -0.942988  -0.958029  -1.026668\n",
      "x0,y6  -9.507184 -3.100000  -0.943030  -0.930575\n",
      "x0,y7  -5.000000 -3.100000  -0.935001  -0.932482\n",
      "x0,y8  -5.000000 -0.901374  -0.910167  -5.000000\n",
      "x1,y0  -0.198205 -0.287037  -5.001000  -0.200219\n",
      "x1,y1  -1.037673 -1.043641 -12.699220  -1.044114\n",
      "x1,y2  -5.891000 -1.034983  -1.156195  -1.041124\n",
      "x1,y3  -1.005975 -5.891000  -1.010354  -1.011523\n",
      "x1,y4  -0.972590 -0.993564  -0.975622  -0.971671\n",
      "x1,y5  -1.051957 -5.891000  -0.971614  -3.100000\n",
      "x1,y6  -0.100000 -3.100000  -0.101000  -3.100000\n",
      "x1,y7  -0.100000 -0.100000  -3.100000  -0.100000\n",
      "x1,y8  -0.866487 -3.100000  -3.100000  -5.000000\n",
      "x2,y0  -3.100000 -1.051069 -13.650178  -1.053221\n",
      "x2,y1  -1.064324 -1.067367  -1.145487  -1.067795\n",
      "x2,y2  -1.054122 -1.052628  -1.048965 -10.664552\n",
      "x2,y3  -0.197857 -3.100000  -0.199682  -0.282651\n",
      "x2,y4  -0.914646 -0.913280  -3.100000  -3.100000\n",
      "x2,y5  -0.100000 -0.194556  -0.100000  -3.100000\n",
      "x2,y6  -3.100000 -0.100000  -3.100000  -0.100000\n",
      "x2,y7  -3.100000 -0.100000  -3.100000  -3.100000\n",
      "x2,y8  -0.101000  0.000000  -0.100000   0.000000\n",
      "x3,y0  -1.085223 -1.079661 -13.569171  -1.082260\n",
      "x3,y1  -1.072834 -1.072657  -1.190034  -1.070011\n",
      "x3,y2  -1.060200 -5.891000  -1.059572  -5.891000\n",
      "x3,y3  -3.100000 -3.100000  -0.100000  -0.193514\n",
      "x3,y4  -0.763326 -0.780851  -3.100000  -0.759913\n",
      "x3,y5  -3.100000 -0.536957  -0.545281  -0.536984\n",
      "x3,y6  -3.100000 -0.191000  -0.191910   3.274591\n",
      "x3,y7  -0.100000 -0.089412  -0.100000  82.489590\n",
      "x3,y8   0.000000  0.000000   0.000000   0.000000\n",
      "x4,y0  -1.065390 -1.067676  -5.000000  -1.065376\n",
      "x4,y1  -1.066815 -1.066239  -1.067345  -3.100000\n",
      "x4,y2  -0.200301 -3.100000  -0.100000  -3.100000\n",
      "x4,y3  -3.100000 -3.100000  -3.100000  -0.191910\n",
      "x4,y4  -0.682795 -0.719081  -5.891000  -0.710386\n",
      "x4,y5  -0.588965 -0.589277  -0.539676  -0.583080\n",
      "x4,y6  -0.386639 -0.419729  -0.352103  -0.388926\n",
      "x4,y7   2.454023 -0.191000  -0.191000  -3.100000\n",
      "x4,y8   0.000000 -0.100000  -0.100000   0.000000\n",
      "x5,y0  -1.081802 -5.891000  -9.503499  -1.078661\n",
      "x5,y1  -1.054533 -1.054731  -1.051245  -3.100000\n",
      "x5,y2  -3.100000 -0.280677  -0.100000  -3.100000\n",
      "x5,y3  -3.100000 -0.101000  -5.891000  -0.191900\n",
      "x5,y4  -0.777841 -0.759275  -5.891010  -0.746403\n",
      "x5,y5  -0.634597 -3.100000  -0.642305  -0.631459\n",
      "x5,y6  -0.533127 -0.538041  -0.541221  -0.535073\n",
      "x5,y7  -0.449113 -0.420397  -0.482032  -0.478846\n",
      "x5,y8  -3.100000 -0.481997  -0.536199  -5.000000\n",
      "x6,y0  -0.100000 -0.191000  -5.000000  -0.100000\n",
      "x6,y1  -1.003496 -5.891010  -3.100000  -1.008731\n",
      "x6,y2  -5.891000 -0.954715  -0.948614  -0.961713\n",
      "x6,y3  -5.891010 -0.908167  -0.918337  -0.920198\n",
      "x6,y4  -0.858112 -0.841883  -0.846677  -5.891000\n",
      "x6,y5  -0.100000 -0.100000  -0.100000  -0.194191\n",
      "x6,y6  -0.586313 -0.591176  -3.100000  -0.584786\n",
      "x6,y7  -0.533753 -0.538591  -0.538551  -0.535940\n",
      "x6,y8  -0.554646 -0.536975  -0.538037  -5.000000\n",
      "x7,y0  -3.100000 -0.941244  -9.505874  -3.100000\n",
      "x7,y1  -0.100000 -0.193507  -0.150000  -0.101000\n",
      "x7,y2  -0.929404 -0.952249  -3.100000  -0.938740\n",
      "x7,y3  -0.906269 -0.887884  -0.891995  -0.902853\n",
      "x7,y4  -0.863220 -0.843444  -0.845513  -0.858732\n",
      "x7,y5  -3.100000 -3.100000  -0.791422  -0.765949\n",
      "x7,y6  -0.633053 -3.100000  -0.641954  -0.675049\n",
      "x7,y7  -0.587386 -0.590538  -0.591192  -0.632177\n",
      "x7,y8  -0.588264 -5.890000  -0.540255 -13.557320\n",
      "x8,y0  -0.950602 -9.508179  -9.508967  -0.949248\n",
      "x8,y1  -3.100000 -9.508909  -0.929120  -0.939519\n",
      "x8,y2  -0.904966 -9.507161  -0.910963  -0.911876\n",
      "x8,y3  -0.886300 -5.000000  -0.868791  -0.864952\n",
      "x8,y4  -0.862834 -5.000000  -0.856330  -3.100000\n",
      "x8,y5  -0.100000 -5.000000  -0.100000  -3.100000\n",
      "x8,y6  -0.100000 -5.000000  -3.100000  -0.101000\n",
      "x8,y7  -0.633613 -5.000000  -3.100000  -3.100000\n",
      "x8,y8  -0.100000 -5.000000  -0.192738  -5.000000\n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "          y0        y1        y2        y3        y4        y5        y6  \\\n",
      "x0 -1.044893 -1.035993 -0.100000 -0.994151 -0.971168 -0.942988 -0.930575   \n",
      "x1 -0.198205 -1.037673 -1.034983 -1.005975 -0.971671 -0.971614 -0.100000   \n",
      "x2 -1.051069 -1.064324 -1.048965 -0.197857 -0.913280 -0.100000 -0.100000   \n",
      "x3 -1.079661 -1.070011 -1.059572 -0.100000 -0.759913 -0.536957  3.274591   \n",
      "x4 -1.065376 -1.066239 -0.100000 -0.191910 -0.682795 -0.539676 -0.352103   \n",
      "x5 -1.078661 -1.051245 -0.100000 -0.101000 -0.746403 -0.631459 -0.533127   \n",
      "x6 -0.100000 -1.003496 -0.948614 -0.908167 -0.841883 -0.100000 -0.584786   \n",
      "x7 -0.941244 -0.100000 -0.929404 -0.887884 -0.843444 -0.765949 -0.633053   \n",
      "x8 -0.949248 -0.929120 -0.904966 -0.864952 -0.856330 -0.100000 -0.100000   \n",
      "\n",
      "           y7        y8  \n",
      "x0  -0.932482 -0.901374  \n",
      "x1  -0.100000 -0.866487  \n",
      "x2  -0.100000  0.000000  \n",
      "x3  82.489590  0.000000  \n",
      "x4   2.454023  0.000000  \n",
      "x5  -0.420397 -0.481997  \n",
      "x6  -0.533753 -0.536975  \n",
      "x7  -0.587386 -0.540255  \n",
      "x8  -0.633613 -0.100000  \n",
      "\n",
      "BEST ACTIONS:\n",
      "       y0     y1    y2     y3     y4     y5     y6     y7    y8\n",
      "x0  Right   Left  Left  Right  Right   Down  Right  Right  Down\n",
      "x1     Up     Up  Down     Up  Right   Left     Up     Up    Up\n",
      "x2   Down     Up  Left     Up   Down     Up   Down   Down  Down\n",
      "x3   Down  Right  Left   Left  Right   Down  Right  Right    Up\n",
      "x4  Right   Down  Left  Right     Up   Left   Left     Up    Up\n",
      "x5  Right   Left  Left   Down  Right  Right     Up   Down  Down\n",
      "x6     Up     Up  Left   Down   Down     Up  Right     Up  Down\n",
      "x7   Down     Up    Up   Down   Down  Right     Up     Up  Left\n",
      "x8  Right   Left    Up  Right   Left     Up     Up     Up    Up\n",
      "\n",
      "STEPS: \n",
      "   [[3, 1], [2, 1], [2, 2], [1, 2], [2, 2], [3, 2], [2, 2], [1, 2], [1, 3], [0, 3], [1, 3], [1, 2], [1, 1], [1, 2], [1, 3], [1, 4], [2, 4], [1, 4], [2, 4], [3, 4], [4, 4], [4, 5], [3, 5], [3, 6], [3, 7], [3, 8]]\n",
      "\n",
      "PATH:\n",
      "   y0  y1         y2       y3     y4  y5  y6  y7  y8\n",
      "x0  -   -          -       10      -   -   -   -   -\n",
      "x1  -  13  4,8,12,14  9,11,15  16,18   -   -   -   -\n",
      "x2  -   2      3,5,7        -  17,19   -   -   -   -\n",
      "x3  0   1          6        -     20  23  24  25  26\n",
      "x4  -   -          -        -     21  22   -   -   -\n",
      "x5  -   -          -        -      -   -   -   -   -\n",
      "x6  -   -          -        -      -   -   -   -   -\n",
      "x7  -   -          -        -      -   -   -   -   -\n",
      "x8  -   -          -        -      -   -   -   -   -\n"
     ]
    }
   ],
   "source": [
    "# apply agent learning\n",
    "episodes_list, best_episode, src = eae.agent_learning(learner=learn.SARSALearner, # Learning algorithm\n",
    "                                        environment = deepcopy(environment),  # Agent environment.\n",
    "                                        num_episodes=30,\n",
    "                                        learning_rate=0.1,\n",
    "                                        discount_factor=0.1,    # Near 0, learn to move to the next most rewarding state\n",
    "                                        ratio_exploration=0.05, # Greedy control: Explore 5% of time and explote %95 of time\n",
    "                                        verbose=True)\n",
    "\n",
    "# print learning process\n",
    "eae.print_process_info(best_episode=best_episode, start_point = src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LONG-TERM FOCUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Actions: 169 - Reward: -1794.0\n",
      "EPISODE 2 - Actions: 267 - Reward: -1720.0\n",
      "EPISODE 3 - Actions: 84 - Reward: -450.0\n",
      "EPISODE 4 - Actions: 78 - Reward: -8.0\n",
      "EPISODE 5 - Actions: 301 - Reward: -370.0\n",
      "EPISODE 6 - Actions: 20 - Reward: 50.0\n",
      "EPISODE 7 - Actions: 150 - Reward: -50.0\n",
      "EPISODE 8 - Actions: 51 - Reward: 0.0\n",
      "EPISODE 9 - Actions: 64 - Reward: 6.0\n",
      "EPISODE 10 - Actions: 291 - Reward: -330.0\n",
      "EPISODE 11 - Actions: 60 - Reward: 40.0\n",
      "EPISODE 12 - Actions: 68 - Reward: 32.0\n",
      "EPISODE 13 - Actions: 56 - Reward: 44.0\n",
      "EPISODE 14 - Actions: 82 - Reward: -12.0\n",
      "EPISODE 15 - Actions: 88 - Reward: 12.0\n",
      "EPISODE 16 - Actions: 49 - Reward: 2.0\n",
      "EPISODE 17 - Actions: 64 - Reward: 36.0\n",
      "EPISODE 18 - Actions: 49 - Reward: -58.0\n",
      "EPISODE 19 - Actions: 70 - Reward: 0.0\n",
      "EPISODE 20 - Actions: 14 - Reward: 86.0\n",
      "EPISODE 21 - Actions: 36 - Reward: 34.0\n",
      "EPISODE 22 - Actions: 84 - Reward: 16.0\n",
      "EPISODE 23 - Actions: 76 - Reward: 24.0\n",
      "EPISODE 24 - Actions: 54 - Reward: 46.0\n",
      "EPISODE 25 - Actions: 61 - Reward: -10.0\n",
      "EPISODE 26 - Actions: 56 - Reward: -16.0\n",
      "EPISODE 27 - Actions: 70 - Reward: 30.0\n",
      "EPISODE 28 - Actions: 48 - Reward: 22.0\n",
      "EPISODE 29 - Actions: 49 - Reward: 2.0\n",
      "EPISODE 30 - Actions: 68 - Reward: -28.0\n",
      "\n",
      "BEST EPISODE:\n",
      "EPISODE 20\n",
      "\tActions: 14\n",
      "\tReward: 86.0\n",
      "\n",
      "Q_TABLE:\n",
      "State        Up       Down      Left     Right\n",
      "x0,y0 -5.000000  -3.100000 -5.000000 -1.859984\n",
      "x0,y1 -9.552730  -1.994811 -2.152533 -5.899000\n",
      "x0,y2 -9.509000  -0.109810 -0.350253 -0.312272\n",
      "x0,y3 -5.000000  -1.459232 -5.899000 -1.485884\n",
      "x0,y4 -9.603177  -1.324682 -1.336656 -1.287935\n",
      "x0,y5 -5.000000  -1.243132 -1.318015 -1.238533\n",
      "x0,y6 -5.000000  -3.100000 -1.183311 -1.285368\n",
      "x0,y7 -5.000000  -3.100000 -1.014194 -1.041528\n",
      "x0,y8 -5.000000  -0.960638 -0.694486 -5.000000\n",
      "x1,y0 -0.109000  -0.315303 -5.000000 -0.100000\n",
      "x1,y1 -1.961970  -2.052143 -5.899000 -2.004793\n",
      "x1,y2 -3.100000  -1.870291 -1.927535 -1.906992\n",
      "x1,y3 -1.592840  -3.100000 -1.594959 -1.640510\n",
      "x1,y4 -1.293914  -1.333861 -1.319804 -1.313681\n",
      "x1,y5 -1.287884  -5.899000 -1.219964 -3.100000\n",
      "x1,y6 -0.109000  -3.100000 -0.100000 -3.100000\n",
      "x1,y7 -0.100000  -0.100000 -3.100000 -0.100000\n",
      "x1,y8 -0.627621  -3.395200 -3.100000 -5.000000\n",
      "x2,y0 -3.100000  -2.120633 -5.000000 -2.162023\n",
      "x2,y1 -2.107002  -2.156290 -2.279579 -2.176452\n",
      "x2,y2 -2.069835  -2.088746 -2.030359 -3.100000\n",
      "x2,y3 -0.100000  -3.100000 -0.100000 -0.100000\n",
      "x2,y4 -0.877510  -0.836183 -3.100000 -3.100000\n",
      "x2,y5 -0.100000  -0.207910 -0.118712 -3.100000\n",
      "x2,y6 -3.100000  -0.100000 -3.100000 -0.100000\n",
      "x2,y7 -3.100000   3.984861 -3.100000 -3.100000\n",
      "x2,y8  0.000000  26.829000  0.000000  0.000000\n",
      "x3,y0 -2.290162  -2.297461 -5.000000 -2.272317\n",
      "x3,y1 -2.228639  -2.184861 -2.164087 -2.186850\n",
      "x3,y2 -2.122208  -5.899000 -2.168838 -6.169000\n",
      "x3,y3 -3.100000  -3.100000 -0.294309 -0.100000\n",
      "x3,y4 -0.536217  -0.491142 -3.100000  1.062199\n",
      "x3,y5 -3.100000  -0.297010 -0.314047 10.696958\n",
      "x3,y6 -3.100000  -0.199000 -0.206584 41.220648\n",
      "x3,y7 -0.280000  -0.100000 -0.100000 80.655100\n",
      "x3,y8  0.000000   0.000000  0.000000  0.000000\n",
      "x4,y0 -2.112836  -2.040304 -5.000000 -2.104567\n",
      "x4,y1 -2.078349  -2.012726 -2.079565 -5.899000\n",
      "x4,y2 -0.309580  -3.100000 -0.232673 -3.100000\n",
      "x4,y3 -3.100000  -3.100000 -3.100000 -0.100000\n",
      "x4,y4 -0.554005  -0.518564 -3.100000 -0.493761\n",
      "x4,y5 -0.358642  -0.395490 -0.321861 -0.230277\n",
      "x4,y6  5.839710  -0.207100 -0.199810 -0.288100\n",
      "x4,y7  4.632861  -0.199000 -0.199000 -3.100000\n",
      "x4,y8  9.900000   0.000000 -0.100000 -5.000000\n",
      "x5,y0 -1.774210  -5.907100 -5.000000 -1.756281\n",
      "x5,y1 -1.778170  -1.763389 -1.773061 -5.899000\n",
      "x5,y2 -3.100000  -0.100000 -0.334270 -3.100000\n",
      "x5,y3 -3.100000  -0.100000 -3.100000 -0.100000\n",
      "x5,y4 -0.711410  -0.659761 -3.100000 -0.721996\n",
      "x5,y5 -0.474084  -3.100000 -0.539395 -0.551165\n",
      "x5,y6 -0.140535  -0.386604 -0.413438 -0.393967\n",
      "x5,y7 -0.310780  -0.394696 -0.402044 -0.402057\n",
      "x5,y8 -3.100000  -0.550000 -0.563032 -5.000000\n",
      "x6,y0 -0.100000  -0.226249 -5.000000 -0.100000\n",
      "x6,y1 -1.514084  -5.899000 -3.100000 -1.547908\n",
      "x6,y2 -3.100000  -1.290680 -1.302479 -1.352879\n",
      "x6,y3 -3.100000  -1.058239 -1.141348 -1.108888\n",
      "x6,y4 -0.876862  -0.891638 -0.855227 -3.100000\n",
      "x6,y5 -0.100000  -0.100000 -0.100000 -0.100000\n",
      "x6,y6 -0.482876  -0.410646 -3.100000 -0.475033\n",
      "x6,y7 -0.394040  -0.409445 -0.394768 -0.466276\n",
      "x6,y8 -0.410648  -0.315365 -0.305831 -5.000000\n",
      "x7,y0 -3.100000  -1.015823 -5.000000 -3.100000\n",
      "x7,y1 -0.100000  -0.100000 -0.251071 -0.100000\n",
      "x7,y2 -1.270629  -1.233461 -3.100000 -1.191826\n",
      "x7,y3 -1.075478  -1.068959 -1.159228 -1.135240\n",
      "x7,y4 -0.918052  -0.997410 -0.933621 -0.938277\n",
      "x7,y5 -3.100000  -3.100000 -0.738697 -0.760312\n",
      "x7,y6 -0.580497  -3.100000 -0.543820 -0.534629\n",
      "x7,y7 -0.487862  -0.394040 -0.439966 -0.460299\n",
      "x7,y8 -0.385292  -3.100000 -0.417856 -5.000000\n",
      "x8,y0 -0.919390 -13.568712 -5.000000 -0.927942\n",
      "x8,y1 -3.100000  -9.544300 -1.173999 -1.237017\n",
      "x8,y2 -1.231931  -5.000000 -1.182315 -1.167910\n",
      "x8,y3 -1.132143  -5.000000 -1.094312 -1.087233\n",
      "x8,y4 -1.001020  -9.553438 -1.083838 -3.100000\n",
      "x8,y5 -0.109000  -5.000000 -0.100000 -3.100000\n",
      "x8,y6 -0.100000  -5.000000 -3.100000 -0.100000\n",
      "x8,y7 -0.394040  -5.000000 -3.100000 -3.100000\n",
      "x8,y8 -0.100000  -5.000000 -0.100000 -5.000000\n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "          y0        y1        y2        y3        y4         y5         y6  \\\n",
      "x0 -1.859984 -1.994811 -0.109810 -1.459232 -1.287935  -1.238533  -1.183311   \n",
      "x1 -0.100000 -1.961970 -1.870291 -1.592840 -1.293914  -1.219964  -0.100000   \n",
      "x2 -2.120633 -2.107002 -2.030359 -0.100000 -0.836183  -0.100000  -0.100000   \n",
      "x3 -2.272317 -2.164087 -2.122208 -0.100000  1.062199  10.696958  41.220648   \n",
      "x4 -2.040304 -2.012726 -0.232673 -0.100000 -0.493761  -0.230277   5.839710   \n",
      "x5 -1.756281 -1.763389 -0.100000 -0.100000 -0.659761  -0.474084  -0.140535   \n",
      "x6 -0.100000 -1.514084 -1.290680 -1.058239 -0.855227  -0.100000  -0.410646   \n",
      "x7 -1.015823 -0.100000 -1.191826 -1.068959 -0.918052  -0.738697  -0.534629   \n",
      "x8 -0.919390 -1.173999 -1.167910 -1.087233 -1.001020  -0.100000  -0.100000   \n",
      "\n",
      "           y7         y8  \n",
      "x0  -1.014194  -0.694486  \n",
      "x1  -0.100000  -0.627621  \n",
      "x2   3.984861  26.829000  \n",
      "x3  80.655100   0.000000  \n",
      "x4   4.632861   9.900000  \n",
      "x5  -0.310780  -0.550000  \n",
      "x6  -0.394040  -0.305831  \n",
      "x7  -0.394040  -0.385292  \n",
      "x8  -0.394040  -0.100000  \n",
      "\n",
      "BEST ACTIONS:\n",
      "       y0    y1     y2     y3     y4     y5     y6     y7    y8\n",
      "x0  Right  Down   Down   Down  Right  Right   Left   Left  Left\n",
      "x1  Right    Up   Down     Up     Up   Left   Left     Up    Up\n",
      "x2   Down    Up   Left     Up   Down     Up   Down   Down  Down\n",
      "x3  Right  Left     Up  Right  Right  Right  Right  Right    Up\n",
      "x4   Down  Down   Left  Right  Right  Right     Up     Up    Up\n",
      "x5  Right  Down   Down   Down   Down     Up     Up     Up  Down\n",
      "x6     Up    Up   Down   Down   Left     Up   Down     Up  Left\n",
      "x7   Down    Up  Right   Down     Up   Left  Right   Down    Up\n",
      "x8     Up  Left  Right  Right     Up   Left     Up     Up    Up\n",
      "\n",
      "STEPS: \n",
      "   [[4, 0], [4, 1], [3, 1], [2, 1], [2, 2], [1, 2], [1, 3], [1, 4], [2, 4], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8]]\n",
      "\n",
      "PATH:\n",
      "   y0 y1 y2 y3  y4  y5  y6  y7  y8\n",
      "x0  -  -  -  -   -   -   -   -   -\n",
      "x1  -  -  6  7   8   -   -   -   -\n",
      "x2  -  4  5  -   9   -   -   -   -\n",
      "x3  0  3  -  -  10  11  12  13  14\n",
      "x4  1  2  -  -   -   -   -   -   -\n",
      "x5  -  -  -  -   -   -   -   -   -\n",
      "x6  -  -  -  -   -   -   -   -   -\n",
      "x7  -  -  -  -   -   -   -   -   -\n",
      "x8  -  -  -  -   -   -   -   -   -\n"
     ]
    }
   ],
   "source": [
    "# apply agent learning\n",
    "episodes_list, best_episode, src= eae.agent_learning(learner=learn.SARSALearner, # Learning algorithm\n",
    "                                        environment = deepcopy(environment), # Agent environment\n",
    "                                        num_episodes=30,\n",
    "                                        learning_rate=0.1,\n",
    "                                        discount_factor=0.9,    # Near 1, move with a view to maximizing the final reward\n",
    "                                        ratio_exploration=0.05, # Greedy control: Explore 5% of time and explote %95 of time\n",
    "                                        verbose=True)\n",
    "# print learning process\n",
    "eae.print_process_info(best_episode=best_episode, start_point = src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
