{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCEMENT LEARNING WITH Q-LEARNING AND SARSA\n",
    "\n",
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Add the parent directory (where \"modules\" is located) to the Python path\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from modules import environment_creation_functions as ecf\n",
    "from modules import environment as env\n",
    "from modules import learners as learn\n",
    "from modules import environment_agent_execution as eae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD ENVIRONMENT\n",
    "\n",
    "Initial state of the agent is represented in green color, the final or destination state in red and barrier in black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating maze\n",
      "Grid of: 8 x 8\n",
      "Barriers count: 16\n",
      "Source: (6, 0)\n",
      "Destination: (3, 7)\n",
      "Path to destination found.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create environment\n",
    "dim, grid_matrix_positions, barriers, src, dest, reward_matrix  = ecf.create_environment(False, 4, 20)\n",
    "environment = env.Environment(reward_matrix, list(src), list(dest)) # convert states from tuple into list\n",
    "\n",
    "# visualize created environment\n",
    "%matplotlib inline\n",
    "ecf.visualize_environment(dim, barriers, src, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCEMENT LEARNING RULES\n",
    "\n",
    "- **Penalty for Each Step**: -1\n",
    "- **Penalty for Hitting a Barrier** (Black Boxes): -10\n",
    "- **Penalty for Moving Out of Bounds**: -50\n",
    "\n",
    "### Q-LEARNER ALGORITHM\n",
    "\n",
    "- Algorithm pseudocode:\n",
    "\n",
    "   ```\n",
    "   Q(,) = 0 # Policy Initialization\n",
    "   For each episode do:\n",
    "        `s` = initial state\n",
    "        For each step, state s (not final) do:\n",
    "            a = Choose action. Cases:\n",
    "                - random(action) if random() <= exploration_rate \n",
    "                - max_a Q(s,)  if random() > exploration_rate  \n",
    "            Execute action a\n",
    "            s = s' # New state\n",
    "            R(s, a) = reward\n",
    "            Q(s, a)' = Q(s, a) + alpha[R(s, a) + (gamma . max_a'Q(s', a')) - Q(s, a)]       \n",
    "        end\n",
    "    end\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHORT-TERM FOCUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Actions: 172 - Reward: -942.0\n",
      "EPISODE 2 - Actions: 104 - Reward: -348.0\n",
      "EPISODE 3 - Actions: 76 - Reward: -252.0\n",
      "EPISODE 4 - Actions: 30 - Reward: 70.0\n",
      "EPISODE 5 - Actions: 258 - Reward: -316.0\n",
      "EPISODE 6 - Actions: 90 - Reward: 0.0\n",
      "EPISODE 7 - Actions: 59 - Reward: -18.0\n",
      "EPISODE 8 - Actions: 281 - Reward: -408.0\n",
      "EPISODE 9 - Actions: 58 - Reward: 22.0\n",
      "EPISODE 10 - Actions: 70 - Reward: 0.0\n",
      "EPISODE 11 - Actions: 110 - Reward: -10.0\n",
      "EPISODE 12 - Actions: 156 - Reward: -56.0\n",
      "EPISODE 13 - Actions: 32 - Reward: 58.0\n",
      "EPISODE 14 - Actions: 106 - Reward: -6.0\n",
      "EPISODE 15 - Actions: 157 - Reward: -136.0\n",
      "EPISODE 16 - Actions: 30 - Reward: 70.0\n",
      "EPISODE 17 - Actions: 240 - Reward: -288.0\n",
      "EPISODE 18 - Actions: 26 - Reward: 74.0\n",
      "EPISODE 19 - Actions: 120 - Reward: -20.0\n",
      "EPISODE 20 - Actions: 36 - Reward: 54.0\n",
      "EPISODE 21 - Actions: 63 - Reward: -62.0\n",
      "EPISODE 22 - Actions: 107 - Reward: -56.0\n",
      "EPISODE 23 - Actions: 74 - Reward: 16.0\n",
      "EPISODE 24 - Actions: 91 - Reward: -40.0\n",
      "EPISODE 25 - Actions: 41 - Reward: 10.0\n",
      "EPISODE 26 - Actions: 61 - Reward: -10.0\n",
      "EPISODE 27 - Actions: 116 - Reward: -36.0\n",
      "EPISODE 28 - Actions: 124 - Reward: -44.0\n",
      "EPISODE 29 - Actions: 241 - Reward: -348.0\n",
      "EPISODE 30 - Actions: 43 - Reward: 8.0\n",
      "\n",
      "BEST EPISODE:\n",
      "EPISODE 18\n",
      "\tActions: 26\n",
      "\tReward: 74.0\n",
      "\n",
      "Q_TABLE:\n",
      "State        Up      Down       Left     Right\n",
      "x0,y0  0.000000 -1.101000  -5.000000 -0.100000\n",
      "x0,y1 -5.000000 -0.986602  -1.100000 -0.995344\n",
      "x0,y2 -9.507517 -1.100000  -0.984414 -0.993977\n",
      "x0,y3 -5.000000 -3.785610  -0.973511 -2.091000\n",
      "x0,y4 -5.000000 -0.100000  -0.195891 -0.100000\n",
      "x0,y5 -5.000000 -1.100000  -1.100000 -1.100000\n",
      "x0,y6  0.000000 -0.100000  -0.100000  0.000000\n",
      "x0,y7  0.000000 -0.100000   0.000000  0.000000\n",
      "x1,y0 -1.100000 -0.196826  -5.000000 -0.191000\n",
      "x1,y1 -0.995537 -1.100000  -2.091010 -1.100000\n",
      "x1,y2 -0.195887 -0.193499  -0.192738 -1.100000\n",
      "x1,y3 -0.193499 -0.197283  -1.100000 -0.192738\n",
      "x1,y4 -1.100000 -0.535289  -1.100000 -1.100000\n",
      "x1,y5 -0.100000 -0.101000  -0.100000 -0.100000\n",
      "x1,y6 -1.100000 -0.100000  -1.100000 -0.100000\n",
      "x1,y7 -0.100000 -0.100000  -0.100000  0.000000\n",
      "x2,y0 -2.090000 -0.987152  -5.000000 -2.091000\n",
      "x2,y1 -0.100000 -0.199821  -0.100000 -0.191000\n",
      "x2,y2 -3.785610 -0.895377  -1.100000 -0.894359\n",
      "x2,y3 -1.100000 -2.091000  -0.764165 -0.769502\n",
      "x2,y4 -0.481979 -0.475274  -0.490611 -0.579506\n",
      "x2,y5 -1.100000 -0.340823  -0.277735 -0.346358\n",
      "x2,y6 -0.100000 -0.100000  -0.100000  0.013490\n",
      "x2,y7 -0.100000 26.829000  -0.100000 -5.000000\n",
      "x3,y0 -0.993220 -3.787249  -5.000000 -0.999781\n",
      "x3,y1 -2.091000 -1.011016  -1.010193 -1.013117\n",
      "x3,y2 -0.986051 -0.977063  -0.985661 -2.091000\n",
      "x3,y3 -0.196449 -0.199965  -0.101000 -0.100000\n",
      "x3,y4 -0.276177 -1.100000  -1.100000 -0.343134\n",
      "x3,y5 -0.191010 -1.100000  -0.101010  0.656760\n",
      "x3,y6 -0.100000  0.000000   0.000000 46.387341\n",
      "x3,y7  0.000000  0.000000   0.000000  0.000000\n",
      "x4,y0 -0.280897 -0.280374  -5.000000 -0.284807\n",
      "x4,y1 -1.040182 -1.036941  -2.984689 -1.036411\n",
      "x4,y2 -1.018756 -1.027594  -1.026254 -1.023735\n",
      "x4,y3 -1.100000 -1.022364  -1.024874 -2.091000\n",
      "x4,y4 -0.100000 -0.100000  -0.197509 -1.100000\n",
      "x4,y5 -0.100000 -0.100000  -1.100000 -0.190000\n",
      "x4,y6 -0.001000 -0.100000  -2.091000  0.704262\n",
      "x4,y7 60.645372  0.000000   0.000000  0.000000\n",
      "x5,y0 -2.090000 -1.085660  -9.506747 -1.083803\n",
      "x5,y1 -1.064807 -1.064120  -1.065985 -1.066283\n",
      "x5,y2 -1.042833 -1.041434  -1.043313 -1.046527\n",
      "x5,y3 -1.015723 -1.018256  -1.011028 -1.018565\n",
      "x5,y4 -1.100000 -1.100000  -0.919784 -0.921500\n",
      "x5,y5 -1.100000 -0.672204  -0.692975 -0.702600\n",
      "x5,y6 -0.405520 -0.349158  -0.355690 -0.402455\n",
      "x5,y7  1.101688 -0.100000  -0.191000 -5.000000\n",
      "x6,y0 -1.094364 -1.095344 -17.217720 -1.095348\n",
      "x6,y1 -1.077835 -1.100000  -1.078083 -1.076135\n",
      "x6,y2 -1.053966 -1.062500  -1.055562 -1.053014\n",
      "x6,y3 -1.036035 -3.787435  -1.035824 -1.100000\n",
      "x6,y4 -0.100000 -0.100000  -0.100000 -0.101000\n",
      "x6,y5 -0.542550 -1.100000  -1.100000 -0.532492\n",
      "x6,y6 -0.349139 -0.348357  -0.351409 -0.415496\n",
      "x6,y7 -0.341697 -0.272910  -0.274638 -5.000000\n",
      "x7,y0 -1.096746 -5.000000  -5.000000 -1.100000\n",
      "x7,y1 -0.100000 -5.000000  -0.200807 -0.100000\n",
      "x7,y2 -1.060639 -5.000000  -2.090000 -2.091000\n",
      "x7,y3 -0.288552 -5.000000  -0.282995 -0.201000\n",
      "x7,y4 -1.100000 -5.000000  -2.091000 -1.100000\n",
      "x7,y5 -0.100000  0.000000  -0.100000 -0.100000\n",
      "x7,y6 -0.274620 -5.000000  -1.100000 -0.349158\n",
      "x7,y7 -0.347520 -5.000000  -0.273828 -5.000000\n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "          y0        y1        y2        y3        y4        y5         y6  \\\n",
      "x0  0.000000 -0.986602 -0.984414 -0.973511 -0.100000 -1.100000   0.000000   \n",
      "x1 -0.191000 -0.995537 -0.192738 -0.192738 -0.535289 -0.100000  -0.100000   \n",
      "x2 -0.987152 -0.100000 -0.894359 -0.764165 -0.475274 -0.277735   0.013490   \n",
      "x3 -0.993220 -1.010193 -0.977063 -0.100000 -0.276177  0.656760  46.387341   \n",
      "x4 -0.280374 -1.036411 -1.018756 -1.022364 -0.100000 -0.100000   0.704262   \n",
      "x5 -1.083803 -1.064120 -1.041434 -1.011028 -0.919784 -0.672204  -0.349158   \n",
      "x6 -1.094364 -1.076135 -1.053014 -1.035824 -0.100000 -0.532492  -0.348357   \n",
      "x7 -1.096746 -0.100000 -1.060639 -0.201000 -1.100000  0.000000  -0.274620   \n",
      "\n",
      "           y7  \n",
      "x0   0.000000  \n",
      "x1   0.000000  \n",
      "x2  26.829000  \n",
      "x3   0.000000  \n",
      "x4  60.645372  \n",
      "x5   1.101688  \n",
      "x6  -0.272910  \n",
      "x7  -0.273828  \n",
      "\n",
      "BEST ACTIONS:\n",
      "       y0     y1     y2     y3    y4     y5     y6     y7\n",
      "x0     Up   Down   Left   Left  Down   Down     Up     Up\n",
      "x1  Right     Up   Left  Right  Down     Up   Down  Right\n",
      "x2   Down     Up  Right   Left  Down   Left  Right   Down\n",
      "x3     Up   Left   Down  Right    Up  Right  Right     Up\n",
      "x4   Down  Right     Up   Down    Up     Up  Right     Up\n",
      "x5  Right   Down   Down   Left  Left   Down   Down     Up\n",
      "x6     Up  Right  Right   Left    Up  Right   Down   Down\n",
      "x7     Up     Up     Up  Right    Up   Down     Up   Left\n",
      "\n",
      "Steps: \n",
      "   [[5, 0], [6, 0], [7, 0], [6, 0], [6, 1], [6, 2], [6, 1], [6, 0], [5, 0], [5, 1], [5, 0], [6, 0], [7, 0], [6, 0], [6, 1], [5, 1], [5, 2], [5, 3], [6, 3], [5, 3], [5, 4], [5, 5], [5, 6], [5, 7], [4, 7], [3, 7]]\n",
      "\n",
      "PATH:\n",
      "               y0      y1  y2     y3  y4  y5  y6  y7\n",
      "x0              -       -   -      -   -   -   -   -\n",
      "x1              -       -   -      -   -   -   -   -\n",
      "x2              -       -   -      -   -   -   -   -\n",
      "x3              -       -   -      -   -   -   -  26\n",
      "x4              -       -   -      -   -   -   -  25\n",
      "x5         1,9,11   10,16  17  18,20  21  22  23  24\n",
      "x6  0,2,4,8,12,14  5,7,15   6     19   -   -   -   -\n",
      "x7           3,13       -   -      -   -   -   -   -\n"
     ]
    }
   ],
   "source": [
    "# apply agent learning\n",
    "episodes_list, best_episode, src = eae.agent_learning(learner=learn.QLearner, # Learning algorithm\n",
    "                                        environment = deepcopy(environment),  # Agent environment.\n",
    "                                        num_episodes=30,\n",
    "                                        learning_rate=0.1,\n",
    "                                        discount_factor=0.1,    # Near 0, learn to move to the next most rewarding state\n",
    "                                        ratio_exploration=0.05, # Greedy control: Explore 5% of time and explote %95 of time\n",
    "                                        verbose=True)\n",
    "\n",
    "# print learning process\n",
    "eae.print_process_info(best_episode=best_episode, start_point = src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LONG-TERM FOCUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Actions: 38 - Reward: -116.0\n",
      "EPISODE 2 - Actions: 112 - Reward: -810.0\n",
      "EPISODE 3 - Actions: 87 - Reward: -244.0\n",
      "EPISODE 4 - Actions: 59 - Reward: -166.0\n",
      "EPISODE 5 - Actions: 241 - Reward: -446.0\n",
      "EPISODE 6 - Actions: 102 - Reward: -100.0\n",
      "EPISODE 7 - Actions: 185 - Reward: -322.0\n",
      "EPISODE 8 - Actions: 26 - Reward: 64.0\n",
      "EPISODE 9 - Actions: 109 - Reward: -78.0\n",
      "EPISODE 10 - Actions: 25 - Reward: -4.0\n",
      "EPISODE 11 - Actions: 12 - Reward: 88.0\n",
      "EPISODE 12 - Actions: 84 - Reward: -34.0\n",
      "EPISODE 13 - Actions: 46 - Reward: 44.0\n",
      "EPISODE 14 - Actions: 32 - Reward: 48.0\n",
      "EPISODE 15 - Actions: 24 - Reward: 76.0\n",
      "EPISODE 16 - Actions: 27 - Reward: 14.0\n",
      "EPISODE 17 - Actions: 30 - Reward: 70.0\n",
      "EPISODE 18 - Actions: 52 - Reward: 28.0\n",
      "EPISODE 19 - Actions: 28 - Reward: 52.0\n",
      "EPISODE 20 - Actions: 38 - Reward: 32.0\n",
      "EPISODE 21 - Actions: 56 - Reward: 24.0\n",
      "EPISODE 22 - Actions: 34 - Reward: 56.0\n",
      "EPISODE 23 - Actions: 54 - Reward: 46.0\n",
      "EPISODE 24 - Actions: 16 - Reward: 74.0\n",
      "EPISODE 25 - Actions: 24 - Reward: 76.0\n",
      "EPISODE 26 - Actions: 55 - Reward: -4.0\n",
      "EPISODE 27 - Actions: 20 - Reward: 80.0\n",
      "EPISODE 28 - Actions: 20 - Reward: 80.0\n",
      "EPISODE 29 - Actions: 35 - Reward: 16.0\n",
      "EPISODE 30 - Actions: 28 - Reward: 72.0\n",
      "\n",
      "BEST EPISODE:\n",
      "EPISODE 11\n",
      "\tActions: 12\n",
      "\tReward: 88.0\n",
      "\n",
      "Q_TABLE:\n",
      "State        Up      Down      Left     Right\n",
      "x0,y0 -5.000000 -1.100000 -5.000000 -0.289196\n",
      "x0,y1 -5.000000 -1.190965 -2.090000 -1.131209\n",
      "x0,y2 -5.000000 -1.100000 -1.161633 -1.140887\n",
      "x0,y3 -5.000000 -1.100000 -1.146965 -1.100000\n",
      "x0,y4 -5.000000 -0.100000 -0.100000 -0.100000\n",
      "x0,y5 -5.000000  0.000000 -1.100000 -1.100000\n",
      "x0,y6  0.000000 -0.100000  0.000000  0.000000\n",
      "x0,y7  0.000000  0.000000  0.000000  0.000000\n",
      "x1,y0 -1.100000 -0.109000 -5.000000 -0.289810\n",
      "x1,y1 -1.130308 -2.107100 -2.099000 -2.099000\n",
      "x1,y2 -0.109000 -0.109000 -0.289000 -1.100000\n",
      "x1,y3 -0.100000 -0.100000 -1.100000 -0.100000\n",
      "x1,y4 -1.100000 -0.320994 -1.100000 -2.090000\n",
      "x1,y5 -0.100000 -0.100000 -0.100000 -0.100000\n",
      "x1,y6  0.000000 -0.100000 -1.100000 -0.100000\n",
      "x1,y7  0.000000  0.000000 -0.100000 -5.000000\n",
      "x2,y0 -1.100000 -1.126457 -5.000000 -1.100000\n",
      "x2,y1 -0.190000 -0.100000 -0.251162 -0.100000\n",
      "x2,y2 -1.100000 -0.759778 -2.090000 -0.778388\n",
      "x2,y3 -1.100000 -1.100000 -0.544117 -0.599471\n",
      "x2,y4 -0.372301 -0.400455 -0.326638 -0.369983\n",
      "x2,y5 -1.100000 -0.199000 -0.199000  0.000503\n",
      "x2,y6  0.000000  5.038469 -0.100000  0.000000\n",
      "x2,y7  0.000000  0.000000  0.000000  0.000000\n",
      "x3,y0 -1.069416 -1.100000 -5.000000 -1.111953\n",
      "x3,y1 -1.100000 -1.068885 -1.050224 -1.090819\n",
      "x3,y2 -0.990750 -1.008856 -0.990458 -1.100000\n",
      "x3,y3 -0.100000 -0.109000 -0.253286 -0.190000\n",
      "x3,y4 -0.305174 -1.100000 -1.100000 -0.159075\n",
      "x3,y5 -0.199000 -1.100000 -0.207910  6.550524\n",
      "x3,y6  0.000000  0.000000 -0.100000 56.383746\n",
      "x3,y7  0.000000  0.000000  0.000000  0.000000\n",
      "x4,y0 -0.100000 -0.109810 -5.000000 -0.274017\n",
      "x4,y1 -1.114753 -1.123777 -1.100000 -1.150730\n",
      "x4,y2 -1.070489 -1.074655 -0.998292 -0.998750\n",
      "x4,y3 -2.998100 -0.958854 -0.925712 -1.100000\n",
      "x4,y4 -0.100000 -0.100000 -0.109810 -1.100000\n",
      "x4,y5 -0.100000 -0.100000 -1.100000 -0.100000\n",
      "x4,y6  7.187721 -0.109000 -1.100000 -0.100000\n",
      "x4,y7 26.829000 -0.100000 -0.100000 -5.000000\n",
      "x5,y0 -2.099000 -1.334503 -9.584295 -1.372931\n",
      "x5,y1 -1.240055 -1.177991 -1.231813 -1.213831\n",
      "x5,y2 -1.062677 -1.145110 -1.112661 -1.136581\n",
      "x5,y3 -0.945681 -1.021931 -1.008646 -0.996862\n",
      "x5,y4 -1.100000 -1.100000 -0.762901 -0.807122\n",
      "x5,y5 -1.100000 -0.558058 -0.564133 -0.590132\n",
      "x5,y6 -0.040714 -0.297010 -0.330382 -0.296200\n",
      "x5,y7  2.150900 -0.199810 -0.199000 -5.000000\n",
      "x6,y0 -1.511628 -1.483380 -9.625535 -1.464701\n",
      "x6,y1 -1.240573 -2.099000 -1.265439 -1.294892\n",
      "x6,y2 -1.195420 -1.219483 -1.136443 -1.156418\n",
      "x6,y3 -1.084068 -2.090000 -1.109952 -1.100000\n",
      "x6,y4 -0.100000 -0.100000 -0.100000 -0.100000\n",
      "x6,y5 -0.528597 -1.100000 -1.100000 -0.459577\n",
      "x6,y6 -0.392509 -0.392582 -0.307582 -0.386750\n",
      "x6,y7 -0.296200 -0.297010 -0.321140 -5.000000\n",
      "x7,y0 -1.505862 -5.000000 -5.000000 -2.099000\n",
      "x7,y1 -0.278807 -5.000000 -0.310921 -0.289000\n",
      "x7,y2 -1.157138 -9.538139 -2.099000 -2.099000\n",
      "x7,y3 -0.227660 -5.000000 -0.138139 -0.289000\n",
      "x7,y4 -1.100000 -5.000000 -1.109000 -2.099000\n",
      "x7,y5 -0.228210 -5.000000 -0.100000 -0.100000\n",
      "x7,y6 -0.394040 -9.526002 -1.100000 -0.386750\n",
      "x7,y7 -0.297010 -5.000000 -0.394040 -9.500000\n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "          y0        y1        y2        y3        y4        y5         y6  \\\n",
      "x0 -0.289196 -1.131209 -1.100000 -1.100000 -0.100000  0.000000   0.000000   \n",
      "x1 -0.109000 -1.130308 -0.109000 -0.100000 -0.320994 -0.100000   0.000000   \n",
      "x2 -1.100000 -0.100000 -0.759778 -0.544117 -0.326638  0.000503   5.038469   \n",
      "x3 -1.069416 -1.050224 -0.990458 -0.100000 -0.159075  6.550524  56.383746   \n",
      "x4 -0.100000 -1.100000 -0.998292 -0.925712 -0.100000 -0.100000   7.187721   \n",
      "x5 -1.334503 -1.177991 -1.062677 -0.945681 -0.762901 -0.558058  -0.040714   \n",
      "x6 -1.464701 -1.240573 -1.136443 -1.084068 -0.100000 -0.459577  -0.307582   \n",
      "x7 -1.505862 -0.278807 -1.157138 -0.138139 -1.100000 -0.100000  -0.386750   \n",
      "\n",
      "          y7  \n",
      "x0   0.00000  \n",
      "x1   0.00000  \n",
      "x2   0.00000  \n",
      "x3   0.00000  \n",
      "x4  26.82900  \n",
      "x5   2.15090  \n",
      "x6  -0.29620  \n",
      "x7  -0.29701  \n",
      "\n",
      "BEST ACTIONS:\n",
      "       y0     y1    y2    y3     y4     y5     y6  y7\n",
      "x0  Right  Right  Down  Down   Down   Down     Up  Up\n",
      "x1   Down     Up    Up    Up   Down     Up     Up  Up\n",
      "x2     Up   Down  Down  Left   Left  Right   Down  Up\n",
      "x3     Up   Left  Left    Up  Right  Right  Right  Up\n",
      "x4     Up   Left  Left  Left     Up     Up     Up  Up\n",
      "x5   Down   Down    Up    Up   Left   Down     Up  Up\n",
      "x6  Right     Up  Left    Up     Up  Right   Left  Up\n",
      "x7     Up     Up    Up  Left     Up   Left  Right  Up\n",
      "\n",
      "Steps: \n",
      "   [[5, 0], [5, 1], [4, 1], [4, 2], [3, 2], [2, 2], [2, 3], [2, 4], [3, 4], [3, 5], [3, 6], [3, 7]]\n",
      "\n",
      "PATH:\n",
      "   y0 y1 y2 y3 y4  y5  y6  y7\n",
      "x0  -  -  -  -  -   -   -   -\n",
      "x1  -  -  -  -  -   -   -   -\n",
      "x2  -  -  6  7  8   -   -   -\n",
      "x3  -  -  5  -  9  10  11  12\n",
      "x4  -  3  4  -  -   -   -   -\n",
      "x5  1  2  -  -  -   -   -   -\n",
      "x6  0  -  -  -  -   -   -   -\n",
      "x7  -  -  -  -  -   -   -   -\n"
     ]
    }
   ],
   "source": [
    "# apply agent learning\n",
    "episodes_list, best_episode, src= eae.agent_learning(learner=learn.QLearner, # Learning algorithm\n",
    "                                        environment = deepcopy(environment), # Agent environment\n",
    "                                        num_episodes=30,\n",
    "                                        learning_rate=0.1,\n",
    "                                        discount_factor=0.9,    # Near 1, move with a view to maximizing the final reward\n",
    "                                        ratio_exploration=0.05, # Greedy control: Explore 5% of time and explote %95 of time\n",
    "                                        verbose=True)\n",
    "# print learning process\n",
    "eae.print_process_info(best_episode=best_episode, start_point = src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA ALGORITHM\n",
    "\n",
    "- Algorithm pseudocode:\n",
    "\n",
    "   ```\n",
    "   Q(,) = 0 # Policy Initialization\n",
    "   For each episode do:\n",
    "        `s` = initial state\n",
    "        For each step, state s (not final) do:\n",
    "            a = Choose action. Cases:\n",
    "                - random(action) if random() <= exploration_rate \n",
    "                - max_a Q(s,)  if random() > exploration_rate  \n",
    "            Execute action a\n",
    "            s = s' # New state\n",
    "            R(s, a) = reward\n",
    "            a' = Choose new action. Cases:\n",
    "                - random(action) if random() <= exploration_rate \n",
    "                - max_a' Q(s',)  if random() > exploration_rate   \n",
    "            Q(s, a)' = Q(s, a) + alpha[R(s, a) + (gamma . Q(s', a')) - Q(s, a)]       \n",
    "        end\n",
    "    end\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHORT-TERM FOCUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Actions: 100 - Reward: -730.0\n",
      "EPISODE 2 - Actions: 146 - Reward: -588.0\n",
      "EPISODE 3 - Actions: 137 - Reward: -254.0\n",
      "EPISODE 4 - Actions: 144 - Reward: -438.0\n",
      "EPISODE 5 - Actions: 122 - Reward: -52.0\n",
      "EPISODE 6 - Actions: 50 - Reward: 50.0\n",
      "EPISODE 7 - Actions: 224 - Reward: -222.0\n",
      "EPISODE 8 - Actions: 136 - Reward: -36.0\n",
      "EPISODE 9 - Actions: 95 - Reward: -182.0\n",
      "EPISODE 10 - Actions: 118 - Reward: -38.0\n",
      "EPISODE 11 - Actions: 146 - Reward: -46.0\n",
      "EPISODE 12 - Actions: 26 - Reward: 74.0\n",
      "EPISODE 13 - Actions: 104 - Reward: -4.0\n",
      "EPISODE 14 - Actions: 72 - Reward: 28.0\n",
      "EPISODE 15 - Actions: 48 - Reward: 52.0\n",
      "EPISODE 16 - Actions: 85 - Reward: -54.0\n",
      "EPISODE 17 - Actions: 155 - Reward: -104.0\n",
      "EPISODE 18 - Actions: 52 - Reward: 38.0\n",
      "EPISODE 19 - Actions: 34 - Reward: 66.0\n",
      "EPISODE 20 - Actions: 94 - Reward: -4.0\n",
      "EPISODE 21 - Actions: 24 - Reward: 76.0\n",
      "EPISODE 22 - Actions: 125 - Reward: -74.0\n",
      "EPISODE 23 - Actions: 70 - Reward: 20.0\n",
      "EPISODE 24 - Actions: 40 - Reward: 60.0\n",
      "EPISODE 25 - Actions: 90 - Reward: 0.0\n",
      "EPISODE 26 - Actions: 80 - Reward: 20.0\n",
      "EPISODE 27 - Actions: 74 - Reward: -72.0\n",
      "EPISODE 28 - Actions: 66 - Reward: 24.0\n",
      "EPISODE 29 - Actions: 28 - Reward: 72.0\n",
      "EPISODE 30 - Actions: 119 - Reward: -88.0\n",
      "\n",
      "BEST EPISODE:\n",
      "EPISODE 21\n",
      "\tActions: 24\n",
      "\tReward: 76.0\n",
      "\n",
      "Q_TABLE:\n",
      "State        Up       Down       Left     Right\n",
      "x0,y0  0.000000  -1.100000   0.000000 -0.100000\n",
      "x0,y1 -9.504170  -0.724597  -1.100000 -0.716959\n",
      "x0,y2 -9.504798  -2.091000  -0.717902 -0.717365\n",
      "x0,y3 -5.000000  -1.100000  -0.721225 -1.100000\n",
      "x0,y4 -5.000000  -0.100000  -0.101110 -0.100000\n",
      "x0,y5 -5.000000  -1.100000  -1.100000 -1.100000\n",
      "x0,y6  0.000000   0.000000  -0.100000 -0.190000\n",
      "x0,y7 -5.000000  -0.191000  -1.100000 -5.000000\n",
      "x1,y0 -1.100000  -0.101000   0.000000 -0.100000\n",
      "x1,y1 -0.717400  -1.100000  -1.100000 -2.091000\n",
      "x1,y2 -0.192739  -0.196750  -0.191000 -1.100000\n",
      "x1,y3 -0.100000  -0.100000  -1.100000 -0.100000\n",
      "x1,y4 -1.100000  -0.537661  -1.100000 -1.100000\n",
      "x1,y5 -0.100000  -0.100000  -0.101000 -0.100000\n",
      "x1,y6 -1.100000  -0.100000  -1.100000 -0.100000\n",
      "x1,y7 -0.101000  -0.100000  -0.190000 -5.000000\n",
      "x2,y0 -1.100000  -1.047863  -5.000000 -1.100000\n",
      "x2,y1 -0.100000  -0.100000  -0.199747 -0.100000\n",
      "x2,y2 -2.091000  -0.935930  -1.100000 -0.945498\n",
      "x2,y3 -1.100000  -1.100000  -0.800033 -0.822105\n",
      "x2,y4 -0.536355  -0.583782  -0.548838 -0.578631\n",
      "x2,y5 -1.100000  -0.323860  -0.279040 -0.271000\n",
      "x2,y6 -0.100000   0.489328  -0.100000 -0.100000\n",
      "x2,y7 -0.101000   9.900000   0.000000  0.000000\n",
      "x3,y0 -1.055245  -1.100000 -13.553492 -1.057134\n",
      "x3,y1 -2.091000  -1.041409  -1.039898 -1.042136\n",
      "x3,y2 -1.019053  -1.018423  -1.019478 -1.100000\n",
      "x3,y3 -0.100000  -0.100000  -0.101010 -0.101000\n",
      "x3,y4 -0.420984  -1.100000  -1.100000 -0.434383\n",
      "x3,y5 -0.100000  -1.100000  -0.191000  2.703738\n",
      "x3,y6 -0.101000   0.000000   0.000000 76.351975\n",
      "x3,y7  0.000000   0.000000   0.000000  0.000000\n",
      "x4,y0 -0.194178  -0.100000  -5.000000 -0.100000\n",
      "x4,y1 -1.055154  -1.055303  -2.091000 -1.055096\n",
      "x4,y2 -1.041512  -1.048239  -1.046870 -1.044230\n",
      "x4,y3 -1.100000  -1.038368  -1.033597 -2.982900\n",
      "x4,y4 -0.192746  -0.100000  -0.100000 -2.091000\n",
      "x4,y5 -0.171284  -0.194759  -1.100000 -0.101000\n",
      "x4,y6  1.300166  -0.101000  -1.100000 -0.100000\n",
      "x4,y7 46.387341  -0.100000  -0.100000 -5.000000\n",
      "x5,y0 -1.100000  -1.066385  -9.550000 -1.068328\n",
      "x5,y1 -1.083697  -1.076945  -1.075049 -1.074711\n",
      "x5,y2 -1.063546  -1.063820  -1.064672 -1.061985\n",
      "x5,y3 -1.038876  -1.036252  -1.042416 -1.041113\n",
      "x5,y4 -1.100000  -1.100000  -0.953475 -0.964320\n",
      "x5,y5 -2.091000  -0.745624  -0.769630 -0.737347\n",
      "x5,y6 -0.452764  -0.420396  -0.424804 -0.466328\n",
      "x5,y7  0.609520  -0.274655  -0.191910 -5.000000\n",
      "x6,y0 -1.164644  -1.101671  -9.508364 -1.100938\n",
      "x6,y1 -1.093790  -1.100000  -1.167440 -1.094645\n",
      "x6,y2 -1.077882  -1.075876  -1.077740 -1.077845\n",
      "x6,y3 -1.062933  -2.091000  -1.059901 -1.100000\n",
      "x6,y4 -0.101000  -0.100000  -0.100000 -0.100000\n",
      "x6,y5 -0.600115  -2.091000  -1.100000 -0.631607\n",
      "x6,y6 -0.533276  -0.479429  -0.488802 -0.513151\n",
      "x6,y7 -0.413045  -0.351394  -0.417528 -5.000000\n",
      "x7,y0 -1.062387  -5.000000  -5.000000 -1.100000\n",
      "x7,y1 -0.100000  -5.000000  -0.100000 -0.100000\n",
      "x7,y2 -1.054884 -13.565819  -1.100000 -2.091010\n",
      "x7,y3 -0.195078  -5.000000  -0.201089 -0.201000\n",
      "x7,y4 -1.100000  -5.000000  -2.091010 -2.091000\n",
      "x7,y5 -0.101000  -5.000000  -0.201000 -0.191900\n",
      "x7,y6 -0.480965  -5.000000  -1.100000 -0.476790\n",
      "x7,y7 -0.416255  -5.000000  -0.419806 -5.000000\n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "          y0        y1        y2        y3        y4        y5         y6  \\\n",
      "x0  0.000000 -0.716959 -0.717365 -0.721225 -0.100000 -1.100000   0.000000   \n",
      "x1  0.000000 -0.717400 -0.191000 -0.100000 -0.537661 -0.100000  -0.100000   \n",
      "x2 -1.047863 -0.100000 -0.935930 -0.800033 -0.536355 -0.271000   0.489328   \n",
      "x3 -1.055245 -1.039898 -1.018423 -0.100000 -0.420984  2.703738  76.351975   \n",
      "x4 -0.100000 -1.055096 -1.041512 -1.033597 -0.100000 -0.101000   1.300166   \n",
      "x5 -1.066385 -1.074711 -1.061985 -1.036252 -0.953475 -0.737347  -0.420396   \n",
      "x6 -1.100938 -1.093790 -1.075876 -1.059901 -0.100000 -0.600115  -0.479429   \n",
      "x7 -1.062387 -0.100000 -1.054884 -0.195078 -1.100000 -0.101000  -0.476790   \n",
      "\n",
      "           y7  \n",
      "x0  -0.191000  \n",
      "x1  -0.100000  \n",
      "x2   9.900000  \n",
      "x3   0.000000  \n",
      "x4  46.387341  \n",
      "x5   0.609520  \n",
      "x6  -0.351394  \n",
      "x7  -0.416255  \n",
      "\n",
      "BEST ACTIONS:\n",
      "       y0     y1     y2    y3    y4     y5     y6    y7\n",
      "x0     Up  Right  Right  Left  Down   Down     Up  Down\n",
      "x1   Left     Up   Left    Up  Down     Up   Down  Down\n",
      "x2   Down     Up   Down  Left    Up  Right   Down  Down\n",
      "x3     Up   Left   Down    Up    Up  Right  Right    Up\n",
      "x4   Down  Right     Up  Left  Down  Right     Up    Up\n",
      "x5   Down  Right  Right  Down  Left  Right   Down    Up\n",
      "x6  Right     Up   Down  Left  Down     Up   Down  Down\n",
      "x7     Up     Up     Up    Up    Up     Up  Right    Up\n",
      "\n",
      "Steps: \n",
      "   [[6, 1], [6, 2], [7, 2], [6, 2], [6, 1], [5, 1], [6, 1], [6, 2], [6, 3], [5, 3], [5, 2], [6, 2], [5, 2], [4, 2], [4, 1], [3, 1], [3, 2], [2, 2], [2, 3], [2, 4], [3, 4], [3, 5], [3, 6], [3, 7]]\n",
      "\n",
      "PATH:\n",
      "   y0     y1        y2  y3  y4  y5  y6  y7\n",
      "x0  -      -         -   -   -   -   -   -\n",
      "x1  -      -         -   -   -   -   -   -\n",
      "x2  -      -        18  19  20   -   -   -\n",
      "x3  -     16        17   -  21  22  23  24\n",
      "x4  -     15        14   -   -   -   -   -\n",
      "x5  -      6     11,13  10   -   -   -   -\n",
      "x6  0  1,5,7  2,4,8,12   9   -   -   -   -\n",
      "x7  -      -         3   -   -   -   -   -\n"
     ]
    }
   ],
   "source": [
    "# apply agent learning\n",
    "episodes_list, best_episode, src = eae.agent_learning(learner=learn.SARSALearner, # Learning algorithm\n",
    "                                        environment = deepcopy(environment),  # Agent environment.\n",
    "                                        num_episodes=30,\n",
    "                                        learning_rate=0.1,\n",
    "                                        discount_factor=0.1,    # Near 0, learn to move to the next most rewarding state\n",
    "                                        ratio_exploration=0.05, # Greedy control: Explore 5% of time and explote %95 of time\n",
    "                                        verbose=True)\n",
    "\n",
    "# print learning process\n",
    "eae.print_process_info(best_episode=best_episode, start_point = src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LONG-TERM FOCUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Actions: 79 - Reward: -452.0\n",
      "EPISODE 2 - Actions: 32 - Reward: -50.0\n",
      "EPISODE 3 - Actions: 168 - Reward: -622.0\n",
      "EPISODE 4 - Actions: 88 - Reward: -314.0\n",
      "EPISODE 5 - Actions: 149 - Reward: -108.0\n",
      "EPISODE 6 - Actions: 54 - Reward: -52.0\n",
      "EPISODE 7 - Actions: 119 - Reward: -176.0\n",
      "EPISODE 8 - Actions: 74 - Reward: 26.0\n",
      "EPISODE 9 - Actions: 62 - Reward: 28.0\n",
      "EPISODE 10 - Actions: 42 - Reward: 38.0\n",
      "EPISODE 11 - Actions: 22 - Reward: 58.0\n",
      "EPISODE 12 - Actions: 38 - Reward: 52.0\n",
      "EPISODE 13 - Actions: 70 - Reward: -30.0\n",
      "EPISODE 14 - Actions: 36 - Reward: 54.0\n",
      "EPISODE 15 - Actions: 24 - Reward: 76.0\n",
      "EPISODE 16 - Actions: 28 - Reward: 62.0\n",
      "EPISODE 17 - Actions: 66 - Reward: 14.0\n",
      "EPISODE 18 - Actions: 22 - Reward: 78.0\n",
      "EPISODE 19 - Actions: 60 - Reward: 40.0\n",
      "EPISODE 20 - Actions: 112 - Reward: -160.0\n",
      "EPISODE 21 - Actions: 44 - Reward: 46.0\n",
      "EPISODE 22 - Actions: 20 - Reward: 60.0\n",
      "EPISODE 23 - Actions: 37 - Reward: 14.0\n",
      "EPISODE 24 - Actions: 44 - Reward: 46.0\n",
      "EPISODE 25 - Actions: 24 - Reward: 66.0\n",
      "EPISODE 26 - Actions: 20 - Reward: 80.0\n",
      "EPISODE 27 - Actions: 44 - Reward: 46.0\n",
      "EPISODE 28 - Actions: 16 - Reward: 84.0\n",
      "EPISODE 29 - Actions: 48 - Reward: 52.0\n",
      "EPISODE 30 - Actions: 73 - Reward: -32.0\n",
      "\n",
      "BEST EPISODE:\n",
      "EPISODE 28\n",
      "\tActions: 16\n",
      "\tReward: 84.0\n",
      "\n",
      "Q_TABLE:\n",
      "State        Up      Down      Left     Right\n",
      "x0,y0 -5.000000 -1.100000 -5.000000 -0.100000\n",
      "x0,y1 -5.000000 -1.106415 -1.100000 -1.131659\n",
      "x0,y2 -9.535599 -2.099000 -1.431733 -1.227294\n",
      "x0,y3 -5.000000 -2.099000 -1.152837 -1.100000\n",
      "x0,y4  0.000000 -0.100000  0.000000 -0.100000\n",
      "x0,y5  0.000000 -1.100000  0.000000 -1.100000\n",
      "x0,y6  0.000000 -0.100000 -0.100000  0.000000\n",
      "x0,y7  0.000000  0.000000  0.000000  0.000000\n",
      "x1,y0 -1.100000 -0.289618  0.000000 -0.100000\n",
      "x1,y1 -1.158016 -1.109000 -1.100000 -1.100000\n",
      "x1,y2 -0.292829 -0.100000 -0.109000 -2.090000\n",
      "x1,y3 -0.190000 -0.100000 -1.100000 -0.233546\n",
      "x1,y4 -1.100000 -0.665594 -1.109000 -1.100000\n",
      "x1,y5  0.000000 -0.190000 -0.109000 -0.100000\n",
      "x1,y6 -1.100000  0.000000 -1.100000 -0.100000\n",
      "x1,y7  0.000000 -0.100000  0.000000 -9.500000\n",
      "x2,y0 -2.099000 -1.911742 -9.632559 -2.099000\n",
      "x2,y1 -0.190000 -0.289237 -0.199000 -0.297829\n",
      "x2,y2 -2.090000 -1.413457 -2.099883 -1.444113\n",
      "x2,y3 -1.100000 -1.100000 -1.136024 -1.025028\n",
      "x2,y4 -0.587287  0.242026 -0.566947 -0.550695\n",
      "x2,y5 -1.100000  0.023248 -0.313850 -0.361000\n",
      "x2,y6 -0.100000 -0.100000 -0.207100  0.701000\n",
      "x2,y7  0.000000 18.810000 -0.100000 -5.000000\n",
      "x3,y0 -1.884751 -2.099000 -5.000000 -1.919614\n",
      "x3,y1 -2.090000 -1.815572 -1.843861 -1.848934\n",
      "x3,y2 -1.718302 -1.676412 -1.692646 -2.099000\n",
      "x3,y3 -0.270606 -0.109000 -0.290024 -0.206219\n",
      "x3,y4 -0.315662 -1.100000 -2.099000  6.000591\n",
      "x3,y5 -0.207100 -1.100000 -0.199810 26.247932\n",
      "x3,y6 -0.100000  0.000000 -0.100000 67.932751\n",
      "x3,y7  0.000000  0.000000  0.000000  0.000000\n",
      "x4,y0 -0.454688 -0.319900 -5.000000 -0.520824\n",
      "x4,y1 -1.917862 -1.850462 -3.015221 -1.887301\n",
      "x4,y2 -1.816178 -1.820476 -1.750730 -1.803716\n",
      "x4,y3 -2.099000 -1.721234 -1.719168 -2.099000\n",
      "x4,y4 -0.217380 -0.289810 -0.117910 -1.100000\n",
      "x4,y5 -0.100000 -0.109000 -1.100000  0.000000\n",
      "x4,y6  0.000000  0.000000  0.000000 36.733439\n",
      "x4,y7 78.616778  0.000000 -0.044929 -5.000000\n",
      "x5,y0 -3.010684 -2.332358 -5.000000 -2.288776\n",
      "x5,y1 -2.080920 -2.053111 -2.127707 -2.065038\n",
      "x5,y2 -1.923325 -1.911584 -1.911264 -1.898767\n",
      "x5,y3 -1.681339 -1.710090 -1.725347 -1.798198\n",
      "x5,y4 -2.099000 -2.107100 -1.250595 -1.116146\n",
      "x5,y5 -1.100000 -0.696340 -0.702812  1.181348\n",
      "x5,y6 10.855365 -0.207100 -0.341866 -0.271000\n",
      "x5,y7  7.433817 -0.100000 -0.109000 -5.000000\n",
      "x6,y0 -2.521545 -2.522361 -5.000000 -2.540232\n",
      "x6,y1 -2.223536 -3.015304 -2.222147 -2.278424\n",
      "x6,y2 -2.032377 -2.025006 -2.054666 -2.086968\n",
      "x6,y3 -1.911968 -3.007010 -1.891905 -2.099000\n",
      "x6,y4 -0.100000 -0.190000 -0.296017 -0.236931\n",
      "x6,y5 -0.539133 -1.109000 -1.109000 -0.589293\n",
      "x6,y6  0.034888 -0.288910 -0.325792 -0.357656\n",
      "x6,y7  0.231953 -0.199000 -0.199890 -5.000000\n",
      "x7,y0 -2.502012 -5.000000 -5.000000 -3.015110\n",
      "x7,y1 -0.552303 -5.000000 -0.301916 -0.520272\n",
      "x7,y2 -2.064179 -5.000000 -2.099810 -2.109441\n",
      "x7,y3 -0.216010 -5.000000 -0.416964 -0.370000\n",
      "x7,y4 -2.090000 -5.000000 -2.099810 -2.099000\n",
      "x7,y5 -0.223219 -5.000000 -0.199000 -0.100000\n",
      "x7,y6 -0.296200 -5.000000 -1.100000 -0.199810\n",
      "x7,y7 -0.207910 -5.000000 -0.199000 -5.000000\n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "          y0        y1        y2        y3        y4         y5         y6  \\\n",
      "x0 -0.100000 -1.100000 -1.227294 -1.100000  0.000000   0.000000   0.000000   \n",
      "x1  0.000000 -1.100000 -0.100000 -0.100000 -0.665594   0.000000   0.000000   \n",
      "x2 -1.911742 -0.190000 -1.413457 -1.025028  0.242026   0.023248   0.701000   \n",
      "x3 -1.884751 -1.815572 -1.676412 -0.109000  6.000591  26.247932  67.932751   \n",
      "x4 -0.319900 -1.850462 -1.750730 -1.719168 -0.117910   0.000000  36.733439   \n",
      "x5 -2.288776 -2.053111 -1.898767 -1.681339 -1.116146   1.181348  10.855365   \n",
      "x6 -2.521545 -2.222147 -2.025006 -1.891905 -0.100000  -0.539133   0.034888   \n",
      "x7 -2.502012 -0.301916 -2.064179 -0.216010 -2.090000  -0.100000  -0.199810   \n",
      "\n",
      "           y7  \n",
      "x0   0.000000  \n",
      "x1   0.000000  \n",
      "x2  18.810000  \n",
      "x3   0.000000  \n",
      "x4  78.616778  \n",
      "x5   7.433817  \n",
      "x6   0.231953  \n",
      "x7  -0.199000  \n",
      "\n",
      "BEST ACTIONS:\n",
      "       y0    y1     y2     y3     y4     y5     y6    y7\n",
      "x0  Right  Left  Right  Right     Up     Up     Up    Up\n",
      "x1   Left  Left   Down   Down   Down     Up   Down    Up\n",
      "x2   Down    Up   Down  Right   Down   Down  Right  Down\n",
      "x3     Up  Down   Down   Down  Right  Right  Right    Up\n",
      "x4   Down  Down   Left   Left   Left  Right  Right    Up\n",
      "x5  Right  Down  Right     Up  Right  Right     Up    Up\n",
      "x6     Up  Left   Down   Left     Up     Up     Up    Up\n",
      "x7     Up  Left     Up     Up     Up  Right  Right  Left\n",
      "\n",
      "Steps: \n",
      "   [[7, 0], [6, 0], [5, 0], [5, 1], [5, 0], [6, 0], [6, 1], [6, 2], [6, 3], [5, 3], [5, 4], [5, 5], [5, 6], [4, 6], [4, 7], [3, 7]]\n",
      "\n",
      "PATH:\n",
      "       y0 y1 y2  y3  y4  y5  y6  y7\n",
      "x0      -  -  -   -   -   -   -   -\n",
      "x1      -  -  -   -   -   -   -   -\n",
      "x2      -  -  -   -   -   -   -   -\n",
      "x3      -  -  -   -   -   -   -  16\n",
      "x4      -  -  -   -   -   -  14  15\n",
      "x5    3,5  4  -  10  11  12  13   -\n",
      "x6  0,2,6  7  8   9   -   -   -   -\n",
      "x7      1  -  -   -   -   -   -   -\n"
     ]
    }
   ],
   "source": [
    "# apply agent learning\n",
    "episodes_list, best_episode, src= eae.agent_learning(learner=learn.SARSALearner, # Learning algorithm\n",
    "                                        environment = deepcopy(environment), # Agent environment\n",
    "                                        num_episodes=30,\n",
    "                                        learning_rate=0.1,\n",
    "                                        discount_factor=0.9,    # Near 1, move with a view to maximizing the final reward\n",
    "                                        ratio_exploration=0.05, # Greedy control: Explore 5% of time and explote %95 of time\n",
    "                                        verbose=True)\n",
    "# print learning process\n",
    "eae.print_process_info(best_episode=best_episode, start_point = src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
